=== ./.gitignore ===

files/

=== ./provision_common.sh ===

#!/bin/bash

set -e

echo "Running common provisioning tasks..."

# Add host entries to ALL nodes (jumpbox, masters, workers)
cat >> /etc/hosts << 'EOF'
# Kubernetes cluster nodes
192.168.56.40 jumpbox
192.168.56.11 master1
192.168.56.21 worker1
192.168.56.22 worker2
EOF

# Disable swap
swapoff -a
sed -i '/ swap / s/^/#/' /etc/fstab

# Install and configure NTP
apt-get update
apt-get install -y chrony curl wget git net-tools
systemctl enable chrony
systemctl start chrony

echo "Common provisioning complete."

=== ./Vagrantfile ===

# -*- mode: ruby -*-
# vi: set ft=ruby :

require 'yaml'

# Load network configuration from network.yml
network_config = YAML.load_file('ansible/inventory/group_vars/network.yml')

# Define resources
RESOURCES = {
  "master" => { "ram" => 2048, "cpu" => 2, "disk" => "50GB" },
  "worker" => { "ram" => 2048, "cpu" => 1, "disk" => "30GB" },
}
JUMP_RAM = 1024
JUMP_DISK = "20GB"

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/jammy64"
  config.ssh.insert_key = false
  config.vm.synced_folder ".", "/home/vagrant/k8s-project"
  
  # Mount Vagrant's insecure keys into the jumpbox
  config.vm.synced_folder "#{Dir.home}/.vagrant.d/insecure_private_keys", "/home/vagrant/vagrant-keys"

  # Master nodes configuration
  network_config['network']['masters'].each_with_index do |master, index|
    config.vm.define master['name'] do |master_node|
      master_node.vm.network "private_network", ip: master['ip']
      master_node.vm.network "forwarded_port", guest: 22, host: 2711 + index, id: "ssh"
      master_node.vm.hostname = master['name']
      master_node.vm.provider "virtualbox" do |vb|
        vb.memory = RESOURCES["master"]["ram"]
        vb.cpus = RESOURCES["master"]["cpu"]
      end
      master_node.vm.provision "shell", path: "provision_common.sh"
    end
  end

  # Worker nodes configuration
  network_config['network']['workers'].each_with_index do |worker, index|
    config.vm.define worker['name'] do |worker_node|
      worker_node.vm.network "private_network", ip: worker['ip']
      worker_node.vm.network "forwarded_port", guest: 22, host: 2721 + index, id: "ssh"
      worker_node.vm.hostname = worker['name']
      worker_node.vm.provider "virtualbox" do |vb|
        vb.memory = RESOURCES["worker"]["ram"]
        vb.cpus = RESOURCES["worker"]["cpu"]
      end
      worker_node.vm.provision "shell", path: "provision_common.sh"
    end
  end

  # Jumpbox configuration (provision LAST)
  config.vm.define "jumpbox" do |jumpbox|
    jumpbox.vm.network "private_network", ip: network_config['network']['jumpbox']['ip']
    jumpbox.vm.network "forwarded_port", guest: 22, host: 2731, id: "ssh"
    jumpbox.vm.hostname = network_config['network']['jumpbox']['name']
    jumpbox.vm.provider "virtualbox" do |vb|
      vb.memory = JUMP_RAM
      vb.cpus = 1
    end
    
    # Run provisioning on jumpbox too
    jumpbox.vm.provision "shell", path: "provision_common.sh"
    jumpbox.vm.provision "shell", path: "provision_jumpbox.sh"
    jumpbox.vm.provision "shell", path: "scripts/distribute-ssh-keys.sh"
  end

  # Trigger for cluster provisioning (runs after jumpbox is fully provisioned)
  config.trigger.after :up do |trigger|
    trigger.name = "provision-cluster"
    trigger.info = "Running Ansible to provision Kubernetes cluster"
    trigger.run_remote = {
      inline: "cd /home/vagrant/k8s-project/ansible && ansible-playbook -i inventory/hosts.ini playbooks/cluster.yml"
    }
    trigger.only_on = "jumpbox"
  end

end


=== ./combine.sh ===

#!/bin/bash

# Script to combine all files into a single file with path headers
# Usage: ./combine_files.sh [output_file]

OUTPUT_FILE="${1:-combined_output.txt}"

# Function to process each file
process_file() {
    local file="$1"
    echo "=== $file ==="
    echo ""
    cat "$file"
    echo ""
    echo ""
}

# Export the function so it can be used by find
export -f process_file

# Clear the output file if it exists
> "$OUTPUT_FILE"

# Build find command with exclusions
find . \
  -type d \( -name '.vscode' -o -name '.vagrant' -o -name '.qodo' -o -name 'files' \) -prune -o \
  -type f \
  ! -name "$OUTPUT_FILE" \
  ! -name "ansible-tree.md" \
  ! -iname "readme.md" \
  -exec bash -c 'process_file "$0"' {} \; >> "$OUTPUT_FILE"

echo "All files have been combined into: $OUTPUT_FILE"
echo "Total files processed: $(find . \
  -type d \( -name '.vscode' -o -name '.vagrant' -o -name '.qodo' -o -name 'files' \) -prune -o \
  -type f \
  ! -name "$OUTPUT_FILE" \
  ! -name "ansible-tree.md" \
  ! -iname "readme.md" \
  -print | wc -l)"

=== ./hosts.txt ===

192.168.56.11 master1
192.168.56.21 worker1
192.168.56.22 worker2
192.168.56.40 jumpbox

=== ./ansible/inventory/group_vars/workers.yml ===

# Kubelet settings
kubelet_cert_file: "{{ inventory_hostname }}.pem"
kubelet_key_file: "{{ inventory_hostname }}-key.pem"
kube_proxy_cert_file: kube-proxy.pem
kube_proxy_key_file: kube-proxy-key.pem

kubelet_args:
  - --config={{ config_dir }}/kubelet-config.yaml
  - --kubeconfig={{ config_dir }}/kubelet.kubeconfig
  - --tls-cert-file={{ pki_dir }}/{{ kubelet_cert_file }}
  - --tls-private-key-file={{ pki_dir }}/{{ kubelet_key_file }}

# Kube-proxy settings
kube_proxy_args:
  - --config={{ config_dir }}/kube-proxy-config.yaml


=== ./ansible/inventory/group_vars/all.yml ===

# Global paths
base_dir: /etc/kubernetes
pki_dir: /var/lib/kubernetes
manifests_dir: "{{ base_dir }}/manifests"
config_dir: "{{ base_dir }}/configs"

# Certificate destination directories
certificate_dirs:
  base_path: "/home/vagrant/k8s-project/ansible/roles/certificates-generate/files"
  kubernetes: "{{ pki_dir }}" # For cluster-wide certs
  etcd: /etc/etcd # For etcd-specific certs
  kubelet: "{{ pki_dir }}" # For kubelet certs (in pki_dir)
  kube_proxy: "{{ pki_dir }}" # For kube-proxy certs

# etcd settings for single-node cluster
etcd_data_dir: /var/lib/etcd
etcd_cert_dir: "{{ certificate_dirs.etcd }}" # Reference the defined directory

# Binary paths (source from synced ansible/roles/*/files/)
cfssl_binary: /home/vagrant/k8s-project/ansible/roles/certificates-generate/files/cfssl
cfssljson_binary: /home/vagrant/k8s-project/ansible/roles/certificates-generate/files/cfssljson
etcd_binary: /home/vagrant/k8s-project/ansible/roles/etcd/files/etcd
etcdctl_binary: /home/vagrant/k8s-project/ansible/roles/etcd/files/etcdctl
kube_apiserver_binary: /home/vagrant/k8s-project/ansible/roles/control-plane/files/kube-apiserver
kube_controller_manager_binary: /home/vagrant/k8s-project/ansible/roles/control-plane/files/kube-controller-manager
kube_scheduler_binary: /home/vagrant/k8s-project/ansible/roles/control-plane/files/kube-scheduler
kubelet_binary: /home/vagrant/k8s-project/ansible/roles/kubelet/files/kubelet
kube_proxy_binary: /home/vagrant/k8s-project/ansible/roles/kube-proxy/files/kube-proxy
runc_binary: /home/vagrant/k8s-project/ansible/roles/container-runtime/files/runc
containerd_binary: /home/vagrant/k8s-project/ansible/roles/container-runtime/files/containerd
kubectl_binary: /home/vagrant/k8s-project/ansible/roles/networking/files/kubectl
flannel_manifest: /home/vagrant/k8s-project/ansible/roles/cni/files/kube-flannel.yml

# Kubernetes versions and settings
kubernetes_version: 1.34.1
etcd_version: 3.6.0
containerd_version: 2.0.0
cfssl_version: 1.6.5

# Kubernetes API server addresses
kubernetes_public_address: 192.168.56.11 # master1 IP
kubernetes_internal_address: 10.32.0.1 # Kubernetes internal IP (usually first IP of service CIDR)

# Network settings
service_cidr: 10.96.0.0/12
api_server_port: 6443
api_server_endpoint: https://192.168.56.11:6443 # Updated to master1
cluster_name: kubernetes

# System settings
swap_enabled: false
ntp_package: chrony

ssh_public_key_path: /home/vagrant/k8s-project/.vagrant/machines/jumpbox/virtualbox/private_key.pub
common_firewall_ports:
  - 22/tcp
  - 6443/tcp
  - 2379/tcp
  - 2380/tcp

cni_plugin: flannel
cni_manifest_paths:
  flannel: "{{ role_path }}/files/flannel.yml"
  calico: "{{ role_path }}/files/calico.yaml"

cni_configs:
  flannel:
    pod_network_cidr: 10.244.0.0/16
  calico:
    pod_network_cidr: 192.168.0.0/16

pod_network_cidr: "{{ cni_configs[cni_plugin].pod_network_cidr }}"

cni_extra_ports:
  flannel:
    - 8472/udp
  calico:
    - 179/tcp
    - 4789/udp


=== ./ansible/inventory/group_vars/masters.yml ===

# Control plane settings
kube_apiserver_bind_address: 0.0.0.0
kube_apiserver_port: "{{ api_server_port }}"
kube_apiserver_args:
  - --etcd-servers=https://192.168.56.11:2379
  - --client-ca-file={{ pki_dir }}/ca.pem
  - --service-account-key-file={{ pki_dir }}/sa.pub
  - --service-account-signing-key-file={{ pki_dir }}/sa-key.pem
  - --tls-cert-file={{ pki_dir }}/apiserver.pem
  - --tls-private-key-file={{ pki_dir }}/apiserver-key.pem
  - --service-cluster-ip-range={{ service_cidr }}
  - --encryption-provider-config={{ config_dir }}/encryption-config.yaml

kube_controller_manager_args:
  - --cluster-signing-cert-file={{ pki_dir }}/ca.pem
  - --cluster-signing-key-file={{ pki_dir }}/ca-key.pem
  - --service-account-private-key-file={{ pki_dir }}/sa-key.pem
  - --root-ca-file={{ pki_dir }}/ca.pem

kube_scheduler_args:
  - --authentication-kubeconfig={{ config_dir }}/kube-scheduler.kubeconfig
  - --authorization-kubeconfig={{ config_dir }}/kube-scheduler.kubeconfig


=== ./ansible/inventory/group_vars/network.yml ===

network:
  masters:
    - ip: 192.168.56.11
      name: master1
  workers:
    - ip: 192.168.56.21
      name: worker1
    - ip: 192.168.56.22
      name: worker2
  jumpbox:
    ip: 192.168.56.40
    name: jumpbox
  pod_cidr: "{{ pod_network_cidr }}"
  service_cidr: 10.96.0.0/12
  dns_service_ip: "{{ service_cidr | ipaddr('10') }}"


=== ./ansible/inventory/group_vars/etcd.yml ===

etcd_args:
  - --name={{ inventory_hostname }}
  - --data-dir={{ etcd_data_dir }}
  - --client-cert-auth=true
  - --cert-file={{ etcd_cert_dir }}/etcd-server.pem
  - --key-file={{ etcd_cert_dir }}/etcd-server-key.pem
  - --trusted-ca-file={{ pki_dir }}/ca.pem
  - --peer-cert-file={{ etcd_cert_dir }}/etcd-server.pem
  - --peer-key-file={{ etcd_cert_dir }}/etcd-server-key.pem
  - --peer-trusted-ca-file={{ pki_dir }}/ca.pem
  - --initial-cluster=master1=https://192.168.56.11:2380
  - --initial-cluster-state=new
  - --listen-client-urls=https://0.0.0.0:2379
  - --advertise-client-urls=https://{{ ansible_host }}:2379
  - --listen-peer-urls=https://0.0.0.0:2380
  - --initial-advertise-peer-urls=https://{{ ansible_host }}:2380


=== ./ansible/inventory/host_vars/worker1.yml ===

# Worker node-specific settings
node_name: worker1
kubelet_kubeconfig: "{{ config_dir }}/kubelet.kubeconfig"
kube_proxy_kubeconfig: "{{ config_dir }}/kube-proxy.kubeconfig"


=== ./ansible/inventory/host_vars/worker2.yml ===

# Worker node-specific settings
node_name: worker2
kubelet_kubeconfig: "{{ config_dir }}/kubelet.kubeconfig"
kube_proxy_kubeconfig: "{{ config_dir }}/kube-proxy.kubeconfig"


=== ./ansible/inventory/host_vars/master1.yml ===

node_name: master1
api_server_endpoint: https://192.168.56.11:6443

# Control plane kubeconfig paths
kube_apiserver_kubeconfig: "{{ config_dir }}/kube-apiserver.kubeconfig"
kube_controller_manager_kubeconfig: "{{ config_dir }}/kube-controller-manager.kubeconfig"
kube_scheduler_kubeconfig: "{{ config_dir }}/kube-scheduler.kubeconfig"


=== ./ansible/inventory/hosts.ini ===

[masters]
master1 ansible_host=192.168.56.11 ansible_user=vagrant

[etcd]
master1

[workers]
worker1 ansible_host=192.168.56.21 ansible_user=vagrant
worker2 ansible_host=192.168.56.22 ansible_user=vagrant

[jumpboxes]
jumpbox ansible_host=192.168.56.40 ansible_user=vagrant

[all:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no'

=== ./ansible/playbooks/04-control-plane.yml ===

- name: Configure Kubernetes control plane
  hosts: masters
  become: yes
  roles:
    - control-plane


=== ./ansible/playbooks/05-container-runtime.yml ===

- name: Configure container runtime
  hosts: masters:workers
  become: yes
  roles:
    - container-runtime


=== ./ansible/playbooks/00-prerequisites.yml ===

- name: Apply prerequisites to all nodes
  hosts: masters:workers
  become: yes
  roles:
    - common


=== ./ansible/playbooks/07-kubelet.yml ===

- name: Configure kubelet
  hosts: workers
  become: yes
  roles:
    - kubelet


=== ./ansible/playbooks/08-kube-proxy.yml ===

- name: Configure kube-proxy
  hosts: workers
  become: yes
  roles:
    - kube-proxy


=== ./ansible/playbooks/10-networking.yml ===

- name: Deploy CoreDNS
  hosts: masters
  become: yes
  roles:
    - networking


=== ./ansible/playbooks/02-distribute-certificates.yml ===

- name: Distribute certificates to all nodes
  hosts: all:!jumpboxes
  become: yes
  roles:
    - certificates-distribute


=== ./ansible/playbooks/jumpbox_setup.yml ===

- name: Setup jumpbox with binaries
  connection: local
  hosts: jumpboxes
  become: yes
  roles:
    - jumpbox_setup


=== ./ansible/playbooks/06-cni.yml ===

- name: Configure Flannel CNI
  hosts: masters
  become: yes
  roles:
    - cni


=== ./ansible/playbooks/cluster.yml ===

- name: Setup prerequisites
  import_playbook: 00-prerequisites.yml

- name: Generate certificates
  import_playbook: 01-generate-certificates.yml

- name: Distribute certificates
  import_playbook: 02-distribute-certificates.yml

- name: Setup etcd cluster
  import_playbook: 03-etcd-cluster.yml

- name: Setup control plane
  import_playbook: 04-control-plane.yml

- name: Setup container runtime
  import_playbook: 05-container-runtime.yml

- name: Setup CNI
  import_playbook: 06-cni.yml

- name: Setup kubelet
  import_playbook: 07-kubelet.yml

- name: Setup kube-proxy
  import_playbook: 08-kube-proxy.yml

- name: Bootstrap cluster
  import_playbook: 09-bootstrap.yml

- name: Setup networking
  import_playbook: 10-networking.yml

- name: Harden cluster
  import_playbook: 11-hardening.yml


=== ./ansible/playbooks/01-generate-certificates.yml ===

- name: Generate certificates on jumpbox
  hosts: jumpboxes
  become: yes
  roles:
    - certificates-generate
  tags: generate-certs


=== ./ansible/playbooks/09-bootstrap.yml ===

- name: Configure bootstrap
  hosts: masters
  become: yes
  roles:
    - bootstrap


=== ./ansible/playbooks/03-etcd-cluster.yml ===

- name: Configure etcd cluster
  hosts: etcd
  become: yes
  roles:
    - etcd


=== ./ansible/playbooks/11-hardening.yml ===

- name: Apply security hardening
  hosts: masters:etcd
  become: yes
  roles:
    - hardening


=== ./ansible/roles/kubelet/defaults/main.yml ===

# Kubelet paths
kubelet_binary_dest: /usr/local/bin/kubelet
kubelet_config: "{{ config_dir }}/kubelet-config.yaml"
kubelet_service: /etc/systemd/system/kubelet.service


=== ./ansible/roles/kubelet/templates/kubelet.service.j2 ===

[Unit]
Description=Kubernetes Kubelet
Documentation=https://kubernetes.io/docs/
After=containerd.service
Requires=containerd.service

[Service]
ExecStart={{ kubelet_binary_dest }} \
    --node-name={{ node_name }} \
    --tls-cert-file={{ pki_dir }}/{{ kubelet_cert_file }} \
    --tls-private-key-file={{ pki_dir }}/{{ kubelet_key_file }} \
    {% for arg in kubelet_args %} {{ arg }} {% endfor %}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

=== ./ansible/roles/kubelet/templates/kubelet-config.yaml.j2 ===

apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
authorization:
  mode: Webhook
clusterDomain: cluster.local
clusterDNS:
  - "{{ service_cidr | ipaddr('10') }}"
tlsCertFile: "{{ pki_dir }}/{{ kubelet_cert_file }}"
tlsPrivateKeyFile: "{{ pki_dir }}/{{ kubelet_key_file }}"
cgroupDriver: systemd


=== ./ansible/roles/kubelet/meta/main.yml ===

dependencies: []


=== ./ansible/roles/kubelet/handlers/main.yml ===

- name: Restart kubelet
  ansible.builtin.systemd:
    name: kubelet
    state: restarted


=== ./ansible/roles/kubelet/tasks/main.yml ===

- name: Configure kubelet
  include_tasks: configure_kubelet.yml


=== ./ansible/roles/kubelet/tasks/configure_kubelet.yml ===

- name: Copy kubelet binary
  ansible.builtin.copy:
    src: "{{ kubelet_binary }}"
    dest: "{{ kubelet_binary_dest }}"
    mode: "0755"

- name: Apply kubelet configuration
  ansible.builtin.template:
    src: kubelet-config.yaml.j2
    dest: "{{ kubelet_config }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kubelet

- name: Apply kubelet service template
  ansible.builtin.template:
    src: kubelet.service.j2
    dest: "{{ kubelet_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kubelet

- name: Ensure kubelet service is enabled and started
  ansible.builtin.systemd:
    name: kubelet
    enabled: yes
    state: started


=== ./ansible/roles/common/defaults/main.yml ===

# Common packages to install
common_packages:
  - curl
  - apt-transport-https
  - ca-certificates

# Firewall ports to allow
common_firewall_ports:
  - 22/tcp # SSH
  - "{{ api_server_port }}/tcp" # Kubernetes API server
  - 2379/tcp # etcd client
  - 2380/tcp # etcd peer
  - 8472/udp # Flannel VXLAN


=== ./ansible/roles/common/templates/limits.conf.j2 ===

* soft nofile 65536
* hard nofile 65536
* soft nproc 65536
* hard nproc 65536
root soft nofile 65536
root hard nofile 65536
root soft nproc 65536
root hard nproc 65536

=== ./ansible/roles/common/templates/sysctl.conf.j2 ===

# Kubernetes networking requirements
net.ipv4.ip_forward = 1
net.ipv6.conf.all.forwarding = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1

=== ./ansible/roles/common/meta/main.yml ===

dependencies: []


=== ./ansible/roles/common/handlers/main.yml ===

- name: Reload sysctl
  ansible.builtin.command:
    cmd: sysctl --system
  changed_when: true

- name: Restart chrony
  ansible.builtin.systemd:
    name: chrony
    state: restarted


=== ./ansible/roles/common/tasks/configure_firewall.yml ===

- name: Install ufw
  ansible.builtin.apt:
    name: ufw
    state: present

- name: Allow necessary ports
  ansible.builtin.ufw:
    rule: allow
    port: "{{ item.split('/')[0] }}"
    proto: "{{ item.split('/')[1] if '/' in item else 'tcp' }}"
  loop: "{{ common_firewall_ports + (cni_extra_ports[cni_plugin] if cni_plugin in cni_extra_ports else []) }}"

- name: Enable ufw
  ansible.builtin.ufw:
    state: enabled
    policy: deny


=== ./ansible/roles/common/tasks/setup_prerequisites.yml ===


- name: Install common packages
  ansible.builtin.apt:
    name: "{{ common_packages }}"
    state: present
    update_cache: yes

- name: Ensure sysctl.conf template is applied
  ansible.builtin.template:
    src: sysctl.conf.j2
    dest: /etc/sysctl.conf
    owner: root
    group: root
    mode: '0644'
  notify: Reload sysctl

- name: Ensure limits.conf template is applied
  ansible.builtin.template:
    src: limits.conf.j2
    dest: /etc/security/limits.conf
    owner: root
    group: root
    mode: '0644'

- name: Ensure swap is disabled
  ansible.builtin.command:
    cmd: swapoff -a
  changed_when: false

- name: Comment out swap entries in fstab
  ansible.builtin.replace:
    path: /etc/fstab
    regexp: '^([^#].*?\sswap\s+.*)$'
    replace: '# \1'

- name: Ensure chrony is installed
  ansible.builtin.apt:
    name: "{{ ntp_package }}"
    state: present

- name: Ensure chrony is enabled and started
  ansible.builtin.systemd:
    name: chrony
    enabled: yes
    state: started

=== ./ansible/roles/common/tasks/main.yml ===

- name: Setup system prerequisites
  include_tasks: setup_prerequisites.yml

- name: Configure firewall
  include_tasks: configure_firewall.yml


=== ./ansible/roles/container-runtime/defaults/main.yml ===

# Container runtime paths
containerd_binary_dest: /usr/local/bin/containerd
runc_binary_dest: /usr/local/bin/runc
containerd_config: /etc/containerd/config.toml
containerd_service: /etc/systemd/system/containerd.service
containerd_package: containerd.io


=== ./ansible/roles/container-runtime/templates/containerd-config.toml.j2 ===

version = 2
[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
    sandbox_image = "registry.k8s.io/pause:3.9"
    [plugins."io.containerd.grpc.v1.cri".containerd]
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          runtime_type = "io.containerd.runc.v2"
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            SystemdCgroup = true

=== ./ansible/roles/container-runtime/meta/main.yml ===

dependencies: []


=== ./ansible/roles/container-runtime/handlers/main.yml ===

- name: Restart containerd
  ansible.builtin.systemd:
    name: containerd
    state: restarted


=== ./ansible/roles/container-runtime/tasks/configure_containerd.yml ===

- name: Copy containerd binary
  ansible.builtin.copy:
    src: "{{ containerd_binary }}"
    dest: "{{ containerd_binary_dest }}"
    mode: "0755"

- name: Copy runc binary
  ansible.builtin.copy:
    src: "{{ runc_binary }}"
    dest: "{{ runc_binary_dest }}"
    mode: "0755"

- name: Ensure containerd configuration directory exists
  ansible.builtin.file:
    path: /etc/containerd
    state: directory
    owner: root
    group: root
    mode: "0755"

- name: Apply containerd configuration
  ansible.builtin.template:
    src: containerd-config.toml.j2
    dest: "{{ containerd_config }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart containerd

- name: Ensure containerd service file exists
  ansible.builtin.copy:
    content: |
      [Unit]
      Description=containerd container runtime
      Documentation=https://containerd.io
      After=network.target

      [Service]
      ExecStart={{ containerd_binary_dest }} --config {{ containerd_config }}
      Restart=on-failure
      LimitNOFILE=65536

      [Install]
      WantedBy=multi-user.target
    dest: "{{ containerd_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart containerd

- name: Ensure containerd service is enabled and started
  ansible.builtin.systemd:
    name: containerd
    enabled: yes
    state: started


=== ./ansible/roles/container-runtime/tasks/main.yml ===

- name: Configure containerd
  include_tasks: configure_containerd.yml


=== ./ansible/roles/etcd/defaults/main.yml ===

# etcd paths
etcd_binary_dest: /usr/local/bin/etcd
etcdctl_binary_dest: /usr/local/bin/etcdctl
etcd_config: /etc/etcd/etcd.conf
etcd_service: /etc/systemd/system/etcd.service


=== ./ansible/roles/etcd/templates/etcd.conf.j2 ===

ETCD_ARGS="{% for arg in etcd_args %} {{ arg }}{% endfor %}"

=== ./ansible/roles/etcd/templates/etcd.service.j2 ===

[Unit]
Description=etcd
Documentation=https://etcd.io/docs/
After=network.target

[Service]
EnvironmentFile={{ etcd_config }}
ExecStart={{ etcd_binary_dest }} $ETCD_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

=== ./ansible/roles/etcd/meta/main.yml ===

dependencies: []

=== ./ansible/roles/etcd/handlers/main.yml ===

- name: Restart etcd
  ansible.builtin.systemd:
    name: etcd
    state: restarted


=== ./ansible/roles/etcd/tasks/service.yml ===

- name: Apply etcd service template
  ansible.builtin.template:
    src: etcd.service.j2
    dest: "{{ etcd_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart etcd

- name: Ensure etcd service is enabled and started
  ansible.builtin.systemd:
    name: etcd
    enabled: yes
    state: started


=== ./ansible/roles/etcd/tasks/main.yml ===

- name: Configure etcd
  include_tasks: configure.yml

- name: Set up etcd service
  include_tasks: service.yml


=== ./ansible/roles/etcd/tasks/configure.yml ===

- name: Copy etcd binary
  ansible.builtin.copy:
    src: "{{ etcd_binary }}"
    dest: "{{ etcd_binary_dest }}"
    mode: "0755"

- name: Copy etcdctl binary
  ansible.builtin.copy:
    src: "{{ etcdctl_binary }}"
    dest: "{{ etcdctl_binary_dest }}"
    mode: "0755"

- name: Ensure etcd data directory exists
  ansible.builtin.file:
    path: "{{ etcd_data_dir }}"
    state: directory
    owner: root
    group: root
    mode: "0700"

- name: Apply etcd configuration
  ansible.builtin.template:
    src: etcd.conf.j2
    dest: "{{ etcd_config }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart etcd


=== ./ansible/roles/control-plane/defaults/main.yml ===

# Control plane paths
apiserver_binary_dest: /usr/local/bin/kube-apiserver
controller_manager_binary_dest: /usr/local/bin/kube-controller-manager
scheduler_binary_dest: /usr/local/bin/kube-scheduler
apiserver_service: /etc/systemd/system/kube-apiserver.service
controller_manager_service: /etc/systemd/system/kube-controller-manager.service
scheduler_service: /etc/systemd/system/kube-scheduler.service
encryption_config: "{{ config_dir }}/encryption-config.yaml"
kubernetes_cert_file: apiserver.pem
kubernetes_key_file: apiserver-key.pem


=== ./ansible/roles/control-plane/templates/kube-apiserver.service.j2 ===

[Unit]
Description=Kubernetes API Server
Documentation=https://kubernetes.io/docs/
After=network.target

[Service]
ExecStart={{ apiserver_binary_dest }} \
    --bind-address={{ kube_apiserver_bind_address }} \
    --secure-port={{ kube_apiserver_port }} \
    --tls-cert-file={{ pki_dir }}/{{ kubernetes_cert_file }} \
    --tls-private-key-file={{ pki_dir }}/{{ kubernetes_key_file }} \
    {% for arg in kube_apiserver_args %} {{ arg }} {% endfor %}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target


=== ./ansible/roles/control-plane/templates/kube-apiserver.yaml.j2 ===

apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - name: kube-apiserver
    image: k8s.gcr.io/kube-apiserver:v{{ kubernetes_version }}
    command:
    - kube-apiserver
    - --advertise-address={{ ansible_host }}
    - --bind-address={{ kube_apiserver_bind_address }}
    - --secure-port={{ kube_apiserver_port }}
    {% for arg in kube_apiserver_args %}
    - {{ arg }}
    {% endfor %}
    volumeMounts:
    - name: pki
      mountPath: {{ pki_dir }}
      readOnly: true
    - name: config
      mountPath: {{ config_dir }}
      readOnly: true
  volumes:
  - name: pki
    hostPath:
      path: {{ pki_dir }}
  - name: config
    hostPath:
      path: {{ config_dir }}

=== ./ansible/roles/control-plane/templates/kube-controller-manager.service.j2 ===

[Unit]
Description=Kubernetes Controller Manager
Documentation=https://kubernetes.io/docs/
After=network.target

[Service]
ExecStart={{ controller_manager_binary_dest }} \
    {% for arg in kube_controller_manager_args %} {{ arg }} {% endfor %}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

=== ./ansible/roles/control-plane/templates/encryption-config.yaml.j2 ===


apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: "{{ lookup('password', '/dev/shm/encryption_key chars=ascii_letters,digits length=32') | b64encode }}"
      - identity: {}


=== ./ansible/roles/control-plane/templates/kube-scheduler.service.j2 ===

[Unit]
Description=Kubernetes Scheduler
Documentation=https://kubernetes.io/docs/
After=network.target

[Service]
ExecStart={{ scheduler_binary_dest }} \
    {% for arg in kube_scheduler_args %} {{ arg }} {% endfor %}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

=== ./ansible/roles/control-plane/meta/main.yml ===

dependencies: []


=== ./ansible/roles/control-plane/handlers/main.yml ===

# ansible/roles/control-plane/handlers/main.yml
- name: Restart kube-apiserver
  ansible.builtin.systemd:
    name: kube-apiserver
    state: restarted

- name: Restart kube-controller-manager
  ansible.builtin.systemd:
    name: kube-controller-manager
    state: restarted

- name: Restart kube-scheduler
  ansible.builtin.systemd:
    name: kube-scheduler
    state: restarted




=== ./ansible/roles/control-plane/tasks/install_scheduler.yml ===

- name: Copy kube-scheduler binary
  ansible.builtin.copy:
    src: "{{ kube_scheduler_binary }}"
    dest: "{{ scheduler_binary_dest }}"
    mode: "0755"

- name: Apply kube-scheduler service template
  ansible.builtin.template:
    src: kube-scheduler.service.j2
    dest: "{{ scheduler_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kube-scheduler

- name: Ensure kube-scheduler service is enabled and started
  ansible.builtin.systemd:
    name: kube-scheduler
    enabled: yes
    state: started


=== ./ansible/roles/control-plane/tasks/install_apiserver.yml ===

- name: Copy kube-apiserver binary
  ansible.builtin.copy:
    src: "{{ kube_apiserver_binary }}"
    dest: "{{ apiserver_binary_dest }}"
    mode: "0755"

- name: Apply kube-apiserver service template
  ansible.builtin.template:
    src: kube-apiserver.service.j2
    dest: "{{ apiserver_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kube-apiserver

- name: Apply encryption configuration
  ansible.builtin.template:
    src: encryption-config.yaml.j2
    dest: "{{ encryption_config }}"
    owner: root
    group: root
    mode: "0600"

- name: Ensure kube-apiserver service is enabled and started
  ansible.builtin.systemd:
    name: kube-apiserver
    enabled: yes
    state: started


=== ./ansible/roles/control-plane/tasks/main.yml ===

- name: Install kube-apiserver
  include_tasks: install_apiserver.yml

- name: Install kube-controller-manager
  include_tasks: install_controller_manager.yml

- name: Install kube-scheduler
  include_tasks: install_scheduler.yml


=== ./ansible/roles/control-plane/tasks/install_controller_manager.yml ===

- name: Copy kube-controller-manager binary
  ansible.builtin.copy:
    src: "{{ kube_controller_manager_binary }}"
    dest: "{{ controller_manager_binary_dest }}"
    mode: "0755"

- name: Apply kube-controller-manager service template
  ansible.builtin.template:
    src: kube-controller-manager.service.j2
    dest: "{{ controller_manager_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kube-controller-manager

- name: Ensure kube-controller-manager service is enabled and started
  ansible.builtin.systemd:
    name: kube-controller-manager
    enabled: yes
    state: started


=== ./ansible/roles/certificates-generate/defaults/main.yml ===

# Certificate paths
ca_config: "{{ role_path }}/files/ca-config.json"
ca_csr: "{{ role_path }}/files/ca-csr.json"
kubernetes_csr: "{{ role_path }}/files/kubernetes-csr.json"
admin_csr: "{{ role_path }}/files/admin-csr.json"
kube_controller_manager_csr: "{{ role_path }}/files/kube-controller-manager-csr.json"
kube_scheduler_csr: "{{ role_path }}/files/kube-scheduler-csr.json"
kubelet_csr: "{{ role_path }}/files/kubelet-csr.json"
kube_proxy_csr: "{{ role_path }}/files/kube-proxy-csr.json"
etcd_csr: "{{ role_path }}/files/etcd-csr.json"
service_account_csr: "{{ role_path }}/files/service-account-csr.json"

# Certificate output paths
ca_pem: "{{ role_path }}/files/ca.pem"
ca_key_pem: "{{ role_path }}/files/ca-key.pem"
kubernetes_pem: "{{ role_path }}/files/apiserver.pem"
kubernetes_key_pem: "{{ role_path }}/files/apiserver-key.pem"
admin_pem: "{{ role_path }}/files/admin.pem"
admin_key_pem: "{{ role_path }}/files/admin-key.pem"
kube_controller_manager_pem: "{{ role_path }}/files/kube-controller-manager.pem"
kube_controller_manager_key_pem: "{{ role_path }}/files/kube-controller-manager-key.pem"
kube_scheduler_pem: "{{ role_path }}/files/kube-scheduler.pem"
kube_scheduler_key_pem: "{{ role_path }}/files/kube-scheduler-key.pem"
kubelet_pem: "{{ role_path }}/files/kubelet.pem"
kubelet_key_pem: "{{ role_path }}/files/kubelet-key.pem"
etcd_pem: "{{ role_path }}/files/etcd-server.pem"
etcd_key_pem: "{{ role_path }}/files/etcd-server-key.pem"
service_account_pem: "{{ role_path }}/files/sa.pub"
service_account_key_pem: "{{ role_path }}/files/sa-key.pem"

# CFSSL settings
cfssl_binary: "{{ cfssl_binary }}"
cfssljson_binary: "{{ cfssljson_binary }}"
ca_profile: kubernetes


=== ./ansible/roles/certificates-generate/templates/ca-csr.json.j2 ===

{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "CA",
      "ST": "Oregon"
    }
  ]
}

=== ./ansible/roles/certificates-generate/templates/kube-proxy-csr.json.j2 ===

{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "system:node-proxier",
      "OU": "Kubernetes The Hard Way",
      "ST": "Oregon"
    }
  ]
}

=== ./ansible/roles/certificates-generate/templates/service-account-csr.json.j2 ===

{
  "CN": "service-accounts",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "Kubernetes",
      "ST": "Oregon"
    }
  ]
}

=== ./ansible/roles/certificates-generate/templates/ca-config.json.j2 ===

{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": [
          "signing",
          "key encipherment", 
          "server auth",
          "client auth"
        ],
        "expiry": "8760h"
      }
    }
  }
}

=== ./ansible/roles/certificates-generate/templates/kube-scheduler-csr.json.j2 ===

{
  "CN": "system:kube-scheduler",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "San Francisco",
      "O": "system:kube-scheduler",
      "OU": "Kubernetes The Hard Way",
      "ST": "California"
    }
  ]
}

=== ./ansible/roles/certificates-generate/templates/kubernetes-csr.json.j2 ===

{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "Kubernetes",
      "OU": "Kubernetes",
      "ST": "Oregon"
    }
  ],
  "hosts": [
    "127.0.0.1",
    "10.32.0.1",
    {% if kubernetes_public_address is defined %}"{{ kubernetes_public_address }}",{% endif %}
    {% if kubernetes_internal_address is defined %}"{{ kubernetes_internal_address }}",{% endif %}
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster.local"
  ]
}

=== ./ansible/roles/certificates-generate/templates/kube-controller-manager-csr.json.j2 ===

{
  "CN": "system:kube-controller-manager",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "San Francisco",
      "O": "system:kube-controller-manager",
      "OU": "Kubernetes The Hard Way",
      "ST": "California"
    }
  ]
}

=== ./ansible/roles/certificates-generate/templates/admin-csr.json.j2 ===

{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "San Francisco",
      "O": "system:masters",
      "OU": "Kubernetes The Hard Way",
      "ST": "California"
    }
  ]
}

=== ./ansible/roles/certificates-generate/templates/kubelet-csr.json.j2 ===

{
  "CN": "system:node:{{ node_name | default('kubelet') }}",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "San Francisco",
      "O": "system:nodes",
      "OU": "Kubernetes The Hard Way",
      "ST": "California"
    }
  ]
}

=== ./ansible/roles/certificates-generate/templates/etcd-csr.json.j2 ===

{
  "CN": "etcd",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "Portland",
      "O": "etcd",
      "OU": "Kubernetes",
      "ST": "Oregon"
    }
  ]
}

=== ./ansible/roles/certificates-generate/meta/main.yml ===

dependencies: []


=== ./ansible/roles/certificates-generate/handlers/main.yml ===

# No handlers needed for certificates


=== ./ansible/roles/certificates-generate/tasks/generate_kube_proxy_certs.yml ===

- name: Generate kube-proxy certificate and key
  ansible.builtin.shell: "{{ cfssl_binary }} gencert -ca={{ role_path }}/files/ca.pem -ca-key={{ role_path }}/files/ca-key.pem -config={{ ca_config }} -profile={{ ca_profile }} {{ role_path }}/files/kube-proxy-csr.json | {{ cfssljson_binary }} -bare kube-proxy"
  args:
    chdir: "{{ role_path }}/files"
    creates: "{{ role_path }}/files/kube-proxy.pem"
  when: not ansible_check_mode


=== ./ansible/roles/certificates-generate/tasks/generate_worker_certs.yml ===

- name: Generate kubelet certificate and key
  ansible.builtin.shell: "{{ cfssl_binary }} gencert -ca={{ role_path }}/files/ca.pem -ca-key={{ role_path }}/files/ca-key.pem -config={{ ca_config }} -profile={{ ca_profile }} {{ kubelet_csr }} | {{ cfssljson_binary }} -bare kubelet"
  args:
    chdir: "{{ role_path }}/files"
    creates: "{{ role_path }}/files/kubelet.pem"
  when: not ansible_check_mode


=== ./ansible/roles/certificates-generate/tasks/create_csr_files.yml ===

- name: Create CA config file
  ansible.builtin.template:
    src: ca-config.json.j2
    dest: "{{ ca_config }}"
    mode: "0644"

- name: Create CA CSR file
  ansible.builtin.template:
    src: ca-csr.json.j2
    dest: "{{ ca_csr }}"
    mode: "0644"

- name: Create admin CSR file
  ansible.builtin.template:
    src: admin-csr.json.j2
    dest: "{{ admin_csr }}"
    mode: "0644"

- name: Create kubernetes CSR file
  ansible.builtin.template:
    src: kubernetes-csr.json.j2
    dest: "{{ kubernetes_csr }}"
    mode: "0644"

- name: Create kube-controller-manager CSR file
  ansible.builtin.template:
    src: kube-controller-manager-csr.json.j2
    dest: "{{ kube_controller_manager_csr }}"
    mode: "0644"

- name: Create kube-scheduler CSR file
  ansible.builtin.template:
    src: kube-scheduler-csr.json.j2
    dest: "{{ kube_scheduler_csr }}"
    mode: "0644"

- name: Create kubelet CSR file
  ansible.builtin.template:
    src: kubelet-csr.json.j2
    dest: "{{ kubelet_csr }}"
    mode: "0644"

- name: Create kube-proxy CSR file
  ansible.builtin.template:
    src: kube-proxy-csr.json.j2
    dest: "{{ role_path }}/files/kube-proxy-csr.json"
    mode: "0644"

- name: Create etcd CSR file
  ansible.builtin.template:
    src: etcd-csr.json.j2
    dest: "{{ etcd_csr }}"
    mode: "0644"

- name: Create service account CSR file
  ansible.builtin.template:
    src: service-account-csr.json.j2
    dest: "{{ service_account_csr }}"
    mode: "0644"


=== ./ansible/roles/certificates-generate/tasks/generate_etcd_certs.yml ===

- name: Generate etcd certificate and key
  ansible.builtin.shell: "{{ cfssl_binary }} gencert -ca={{ role_path }}/files/ca.pem -ca-key={{ role_path }}/files/ca-key.pem -config={{ ca_config }} -profile={{ ca_profile }} {{ etcd_csr }} | {{ cfssljson_binary }} -bare etcd-server"
  args:
    chdir: "{{ role_path }}/files"
    creates: "{{ role_path }}/files/etcd-server.pem"
  when: not ansible_check_mode


=== ./ansible/roles/certificates-generate/tasks/generate_controller_certs.yml ===

- name: Generate kube-controller-manager certificate and key
  ansible.builtin.shell: "{{ cfssl_binary }} gencert -ca={{ role_path }}/files/ca.pem -ca-key={{ role_path }}/files/ca-key.pem -config={{ ca_config }} -profile={{ ca_profile }} {{ kube_controller_manager_csr }} | {{ cfssljson_binary }} -bare kube-controller-manager"
  args:
    chdir: "{{ role_path }}/files"
    creates: "{{ role_path }}/files/kube-controller-manager.pem"
  when: not ansible_check_mode

- name: Generate kube-scheduler certificate and key
  ansible.builtin.shell: "{{ cfssl_binary }} gencert -ca={{ role_path }}/files/ca.pem -ca-key={{ role_path }}/files/ca-key.pem -config={{ ca_config }} -profile={{ ca_profile }} {{ kube_scheduler_csr }} | {{ cfssljson_binary }} -bare kube-scheduler"
  args:
    chdir: "{{ role_path }}/files"
    creates: "{{ role_path }}/files/kube-scheduler.pem"
  when: not ansible_check_mode


=== ./ansible/roles/certificates-generate/tasks/main.yml ===

- name: Create CSR files
  include_tasks: create_csr_files.yml

- name: Generate CA certificate
  include_tasks: generate_ca.yml
  register: ca_generation_result

- name: Generate kubernetes certificate
  include_tasks: generate_kubernetes_certs.yml
  when: ca_generation_result is succeeded

- name: Generate admin client certificate
  include_tasks: generate_admin_cert.yml
  when: ca_generation_result is succeeded

- name: Generate controller certificates
  include_tasks: generate_controller_certs.yml
  when: ca_generation_result is succeeded

- name: Generate etcd certificates
  include_tasks: generate_etcd_certs.yml
  when: ca_generation_result is succeeded

- name: Generate worker certificates
  include_tasks: generate_worker_certs.yml
  when: ca_generation_result is succeeded

- name: Generate kube-proxy certificates
  include_tasks: generate_kube_proxy_certs.yml
  when: ca_generation_result is succeeded

- name: Generate service account key pair
  include_tasks: generate_service_account_key.yml
  when: ca_generation_result is succeeded


=== ./ansible/roles/certificates-generate/tasks/generate_service_account_key.yml ===

- name: Generate service account key pair
  ansible.builtin.shell: "{{ cfssl_binary }} gencert -ca={{ role_path }}/files/ca.pem -ca-key={{ role_path }}/files/ca-key.pem -config={{ ca_config }} -profile={{ ca_profile }} {{ service_account_csr }} | {{ cfssljson_binary }} -bare sa"
  args:
    chdir: "{{ role_path }}/files"
    creates: "{{ role_path }}/files/sa.pub"
  when: not ansible_check_mode


=== ./ansible/roles/certificates-generate/tasks/generate_admin_cert.yml ===

- name: Generate admin certificate and key
  ansible.builtin.shell: "{{ cfssl_binary }} gencert -ca={{ role_path }}/files/ca.pem -ca-key={{ role_path }}/files/ca-key.pem -config={{ ca_config }} -profile={{ ca_profile }} {{ admin_csr }} | {{ cfssljson_binary }} -bare admin"
  args:
    chdir: "{{ role_path }}/files"
    creates: "{{ role_path }}/files/admin.pem"
  when: not ansible_check_mode


=== ./ansible/roles/certificates-generate/tasks/generate_kubernetes_certs.yml ===

- name: Generate kubernetes certificate and key
  ansible.builtin.shell: "{{ cfssl_binary }} gencert -ca={{ role_path }}/files/ca.pem -ca-key={{ role_path }}/files/ca-key.pem -config={{ ca_config }} -profile={{ ca_profile }} {{ kubernetes_csr }} | {{ cfssljson_binary }} -bare apiserver"
  args:
    chdir: "{{ role_path }}/files"
    creates: "{{ role_path }}/files/apiserver.pem"
  when: not ansible_check_mode


=== ./ansible/roles/certificates-generate/tasks/generate_ca.yml ===

- name: Generate CA certificate and key
  ansible.builtin.shell: "{{ cfssl_binary }} gencert -initca {{ ca_csr }} | {{ cfssljson_binary }} -bare ca"
  args:
    chdir: "{{ role_path }}/files"
    creates: "{{ role_path }}/files/ca.pem"
  when: not ansible_check_mode
  register: ca_generation_result
  failed_when: ca_generation_result.rc != 0

- name: Verify CA certificate was created
  ansible.builtin.stat:
    path: "{{ role_path }}/files/ca.pem"
  register: ca_cert_stat
  when: not ansible_check_mode
  failed_when: not ca_cert_stat.stat.exists

- name: Verify CA key was created
  ansible.builtin.stat:
    path: "{{ role_path }}/files/ca-key.pem"
  register: ca_key_stat
  when: not ansible_check_mode
  failed_when: not ca_key_stat.stat.exists


=== ./ansible/roles/certificates-generate/tasks/generate_serviceaccount_key.yml ===

- name: Generate service account certificate and key
  ansible.builtin.shell: "{{ cfssl_binary }} gencert -ca={{ role_path }}/files/ca.pem -ca-key={{ role_path }}/files/ca-key.pem -config={{ ca_config }} -profile={{ ca_profile }} {{ service_account_csr }} | {{ cfssljson_binary }} -bare service-account"
  args:
    chdir: "{{ role_path }}/files"
    creates: "{{ role_path }}/files/sa.pub"
  when: not ansible_check_mode


=== ./ansible/roles/kube-proxy/defaults/main.yml ===

# Kube-proxy paths
kube_proxy_binary_dest: /usr/local/bin/kube-proxy
kube_proxy_config: "{{ config_dir }}/kube-proxy-config.yaml"
kube_proxy_service: /etc/systemd/system/kube-proxy.service


=== ./ansible/roles/kube-proxy/templates/kube-proxy.service.j2 ===

[Unit]
Description=Kubernetes Kube-Proxy
Documentation=https://kubernetes.io/docs/
After=network.target

[Service]
ExecStart={{ kube_proxy_binary_dest }} \
    {% for arg in kube_proxy_args %} {{ arg }} {% endfor %}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

=== ./ansible/roles/kube-proxy/templates/kube-proxy-config.yaml.j2 ===

apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: iptables
clusterCIDR: "{{ pod_network_cidr }}"

=== ./ansible/roles/kube-proxy/meta/main.yml ===

dependencies: []


=== ./ansible/roles/kube-proxy/handlers/main.yml ===

- name: Restart kube-proxy
  ansible.builtin.systemd:
    name: kube-proxy
    state: restarted


=== ./ansible/roles/kube-proxy/tasks/main.yml ===

- name: Configure kube-proxy
  include_tasks: configure_kube_proxy.yml


=== ./ansible/roles/kube-proxy/tasks/configure_kube_proxy.yml ===

- name: Copy kube-proxy binary
  ansible.builtin.copy:
    src: "{{ kube_proxy_binary }}"
    dest: "{{ kube_proxy_binary_dest }}"
    mode: "0755"

- name: Apply kube-proxy configuration
  ansible.builtin.template:
    src: kube-proxy-config.yaml.j2
    dest: "{{ kube_proxy_config }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kube-proxy

- name: Apply kube-proxy service template
  ansible.builtin.template:
    src: kube-proxy.service.j2
    dest: "{{ kube_proxy_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kube-proxy

- name: Ensure kube-proxy service is enabled and started
  ansible.builtin.systemd:
    name: kube-proxy
    enabled: yes
    state: started


=== ./ansible/roles/cni/defaults/main.yml ===

# Flannel paths
flannel_manifest_dest: "{{ config_dir }}/kube-flannel.yml"
kubectl_binary: "{{ kubectl_binary }}"


=== ./ansible/roles/cni/meta/main.yml ===

dependencies: []


=== ./ansible/roles/cni/handlers/main.yml ===

- name: Apply CNI
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} apply -f /etc/kubernetes/{{ cni_plugin }}.yml"
  changed_when: true


=== ./ansible/roles/cni/tasks/main.yml ===

- name: Copy CNI manifest
  ansible.builtin.copy:
    src: "{{ cni_manifest_paths[cni_plugin] }}"
    dest: "/etc/kubernetes/{{ cni_plugin }}.yml"
    owner: root
    group: root
    mode: "0644"
  when: inventory_hostname == 'master1'

- name: Apply CNI manifest
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} apply -f /etc/kubernetes/{{ cni_plugin }}.yml"
    creates: /var/run/cni-applied
  changed_when: true
  when: inventory_hostname == 'master1'
  notify: Apply CNI


=== ./ansible/roles/bootstrap/defaults/main.yml ===

# Bootstrap settings
bootstrap_token: "{{ lookup('password', '/dev/shm/bootstrap_token chars=ascii_lowercase,digits length=16') }}"
bootstrap_token_id: "{{ bootstrap_token[:6] }}"
bootstrap_token_secret: "{{ bootstrap_token[6:] }}"
bootstrap_kubeconfig: "{{ config_dir }}/bootstrap.kubeconfig"


=== ./ansible/roles/bootstrap/templates/kube-proxy.kubeconfig.j2 ===

apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: {{ pki_dir }}/ca.pem
    server: https://192.168.56.11:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kube-proxy
  name: kube-proxy@kubernetes
current-context: kube-proxy@kubernetes
users:
- name: kube-proxy
  user:
    client-certificate: {{ pki_dir }}/kube-proxy.pem
    client-key: {{ pki_dir }}/kube-proxy-key.pem

=== ./ansible/roles/bootstrap/templates/admin.kubeconfig.j2 ===

apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: {{ pki_dir }}/ca.pem
    server: https://192.168.56.11:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
users:
- name: kubernetes-admin
  user:
    client-certificate: {{ pki_dir }}/admin.pem
    client-key: {{ pki_dir }}/admin-key.pem

=== ./ansible/roles/bootstrap/templates/kubelet.kubeconfig.j2 ===

apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: {{ pki_dir }}/ca.pem
    server: https://192.168.56.11:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubelet
  name: kubelet@kubernetes
current-context: kubelet@kubernetes
users:
- name: kubelet
  user:
    client-certificate: {{ pki_dir }}/kubelet.pem
    client-key: {{ pki_dir }}/kubelet-key.pem

=== ./ansible/roles/bootstrap/templates/bootstrap.kubeconfig.j2 ===

apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: {{ pki_dir }}/ca.pem
    server: https://192.168.56.11:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: bootstrap
  name: bootstrap@kubernetes
current-context: bootstrap@kubernetes
users:
- name: bootstrap
  user:
    token: "{{ bootstrap_token_id }}.{{ bootstrap_token_secret }}"

=== ./ansible/roles/bootstrap/meta/main.yml ===

dependencies: []


=== ./ansible/roles/bootstrap/handlers/main.yml ===

- name: Approve CSRs
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} certificate approve {{ item }}"
  loop: "{{ q('kubernetes.core.k8s', api_version='certificates.k8s.io/v1', kind='CertificateSigningRequest') | json_query('[].metadata.name') }}"
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/roles/bootstrap/tasks/main.yml ===

- name: Configure bootstrap
  include_tasks: configure_bootstrap.yml

- name: Generate kubeconfig files
  include_tasks: generate_kubeconfigs.yml


=== ./ansible/roles/bootstrap/tasks/configure_bootstrap.yml ===

- name: Create bootstrap token
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} -n kube-system create secret generic bootstrap-token-{{ bootstrap_token_id }} --type=bootstrap.kubernetes.io/token --metadata=namespace:kube-system --from-literal=token-id={{ bootstrap_token_id }} --from-literal=token-secret={{ bootstrap_token_secret }} --from-literal=usage-bootstrap-authentication=true --from-literal=usage-bootstrap-signing=true"
  changed_when: true
  when: inventory_hostname == 'master1'

- name: Create bootstrap kubeconfig
  ansible.builtin.template:
    src: bootstrap.kubeconfig.j2
    dest: "{{ bootstrap_kubeconfig }}"
    owner: root
    group: root
    mode: "0600"
  when: inventory_hostname in groups['workers']

- name: Approve CSRs
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} certificate approve {{ item }}"
  loop: "{{ q('kubernetes.core.k8s', api_version='certificates.k8s.io/v1', kind='CertificateSigningRequest') | json_query('[].metadata.name') }}"
  changed_when: true
  when: inventory_hostname == 'master1'
  notify: approve_csrs


=== ./ansible/roles/bootstrap/tasks/generate_kubeconfigs.yml ===

- name: Generate admin kubeconfig
  ansible.builtin.template:
    src: admin.kubeconfig.j2
    dest: "{{ config_dir }}/admin.kubeconfig"
    owner: root
    group: root
    mode: "0600"
  when: inventory_hostname in groups['masters'] or inventory_hostname in groups['jumpbox']

- name: Generate kubelet kubeconfig
  ansible.builtin.template:
    src: kubelet.kubeconfig.j2
    dest: "{{ config_dir }}/kubelet.kubeconfig"
    owner: root
    group: root
    mode: "0600"
  when: inventory_hostname in groups['workers']

- name: Generate kube-proxy kubeconfig
  ansible.builtin.template:
    src: kube-proxy.kubeconfig.j2
    dest: "{{ config_dir }}/kube-proxy.kubeconfig"
    owner: root
    group: root
    mode: "0600"
  when: inventory_hostname in groups['workers']


=== ./ansible/roles/hardening/defaults/main.yml ===

audit_policy: "{{ config_dir }}/audit-policy.yaml"
network_policy: "{{ config_dir }}/default-network-policy.yaml"
rbac_config: "{{ config_dir }}/rbac-config.yaml"
etcd_backup_dir: /var/backups/etcd
etcd_backup_script: /usr/local/bin/etcd-backup.sh


=== ./ansible/roles/hardening/templates/default-network-policy.yaml.j2 ===

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  egress:
  - ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 53
    to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns

=== ./ansible/roles/hardening/templates/rbac-config.yaml.j2 ===

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: restricted-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: User
  name: admin
  apiGroup: rbac.authorization.k8s.io

=== ./ansible/roles/hardening/meta/main.yml ===

dependencies: []


=== ./ansible/roles/hardening/handlers/main.yml ===

- name: Apply RBAC
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} apply -f {{ rbac_config }}"
  changed_when: true

- name: Apply network policy
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} apply -f {{ network_policy }}"
  changed_when: true


=== ./ansible/roles/hardening/tasks/remove_bootstrap.yml ===

- name: Remove bootstrap token secret
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} -n kube-system delete secret bootstrap-token-{{ bootstrap_token_id }}"
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/roles/hardening/tasks/main.yml ===

- name: Remove bootstrap tokens
  include_tasks: remove_bootstrap.yml

- name: Configure audit policy
  include_tasks: configure_audit.yml

- name: Configure network policies
  include_tasks: network_policies.yml

- name: Configure RBAC
  include_tasks: configure_rbac.yml

- name: Configure etcd backup
  include_tasks: etcd_backup.yml


=== ./ansible/roles/hardening/tasks/configure_audit.yml ===

- name: Apply audit policy
  ansible.builtin.copy:
    content: |
      apiVersion: audit.k8s.io/v1
      kind: Policy
      rules:
      - level: Metadata
    dest: "{{ audit_policy }}"
    owner: root
    group: root
    mode: "0644"
  when: inventory_hostname in groups['masters']


=== ./ansible/roles/hardening/tasks/configure_rbac.yml ===

- name: Apply RBAC configuration
  ansible.builtin.template:
    src: rbac-config.yaml.j2
    dest: "{{ rbac_config }}"
    owner: root
    group: root
    mode: "0644"
  notify: Apply RBAC
  when: inventory_hostname == 'master1'

- name: Apply RBAC policy
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} apply -f {{ rbac_config }}"
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/roles/hardening/tasks/network_policies.yml ===

- name: Apply default network policy
  ansible.builtin.template:
    src: default-network-policy.yaml.j2
    dest: "{{ network_policy }}"
    owner: root
    group: root
    mode: "0644"
  notify: Apply network policy
  when: inventory_hostname == 'master1'

- name: Apply network policy
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} apply -f {{ network_policy }}"
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/roles/hardening/tasks/etcd_backup.yml ===

- name: Ensure etcd backup directory exists
  ansible.builtin.file:
    path: "{{ etcd_backup_dir }}"
    state: directory
    owner: root
    group: root
    mode: "0700"
  when: inventory_hostname in groups['etcd']

- name: Create etcd backup script
  ansible.builtin.copy:
    content: |
      #!/bin/bash
      ETCDCTL_API=3 {{ etcdctl_binary }} snapshot save {{ etcd_backup_dir }}/etcd-snapshot-$(date +%Y%m%d%H%M%S).db \
        --cacert={{ pki_dir }}/ca.pem \
        --cert={{ etcd_pem }} \
        --key={{ etcd_key_pem }} \
        --endpoints=https://{{ ansible_host }}:2379
    dest: "{{ etcd_backup_script }}"
    owner: root
    group: root
    mode: "0755"
  when: inventory_hostname in groups['etcd']

- name: Schedule etcd backup cron job
  ansible.builtin.cron:
    name: etcd-backup
    minute: "0"
    hour: "2"
    job: "{{ etcd_backup_script }}"
  when: inventory_hostname in groups['etcd']


=== ./ansible/roles/certificates-distribute/defaults/main.yml ===

# Base directory variables
pki_dir: /var/lib/kubernetes
etcd_cert_dir: /etc/etcd

# Certificate destination paths
ca_pem: "{{ pki_dir }}/ca.pem"
ca_key_pem: "{{ pki_dir }}/ca-key.pem"
kubernetes_pem: "{{ pki_dir }}/apiserver.pem"
kubernetes_key_pem: "{{ pki_dir }}/apiserver-key.pem"
admin_pem: "{{ pki_dir }}/admin.pem"
admin_key_pem: "{{ pki_dir }}/admin-key.pem"
service_account_pem: "{{ pki_dir }}/sa.pub"
service_account_key_pem: "{{ pki_dir }}/sa-key.pem"
kube_controller_manager_pem: "{{ pki_dir }}/kube-controller-manager.pem"
kube_controller_manager_key_pem: "{{ pki_dir }}/kube-controller-manager-key.pem"
kube_scheduler_pem: "{{ pki_dir }}/kube-scheduler.pem"
kube_scheduler_key_pem: "{{ pki_dir }}/kube-scheduler-key.pem"
kubelet_pem: "{{ pki_dir }}/kubelet.pem"
kubelet_key_pem: "{{ pki_dir }}/kubelet-key.pem"
etcd_pem: "{{ etcd_cert_dir }}/server.pem"
etcd_key_pem: "{{ etcd_cert_dir }}/server-key.pem"

# Kubelet certificate filenames
kubelet_cert_file: "{{ inventory_hostname }}.pem"
kubelet_key_file: "{{ inventory_hostname }}-key.pem"


=== ./ansible/roles/certificates-distribute/meta/main.yml ===

dependencies: []

=== ./ansible/roles/certificates-distribute/tasks/main.yml ===

- name: Copy certificates to nodes
  include_tasks: copy_certs.yml


=== ./ansible/roles/certificates-distribute/tasks/copy_certs.yml ===

- name: Ensure certificate directories exist
  ansible.builtin.file:
    path: "{{ item }}"
    state: directory
    owner: root
    group: root
    mode: "0755"
  loop:
    - "{{ pki_dir }}" # /var/lib/kubernetes
    - "{{ etcd_cert_dir }}" # /etc/etcd (for master nodes)
    - "{{ config_dir }}" # /etc/kubernetes/configs (if needed for kubeconfigs)
  when: >
    (item == pki_dir) or
    (item == etcd_cert_dir and inventory_hostname in groups['masters']) or
    (item == config_dir)

- name: Copy cluster-wide certificates to all nodes
  ansible.builtin.copy:
    src: "{{ item.src }}"
    dest: "{{ item.dest }}"
    mode: "{{ item.mode }}"
    owner: root
    group: root
  loop:
    - {
        src: "{{ certificate_dirs.base_path }}/ca.pem",
        dest: "{{ ca_pem }}",
        mode: "0644",
      }
    - {
        src: "{{ certificate_dirs.base_path }}/ca-key.pem",
        dest: "{{ ca_key_pem }}",
        mode: "0600",
      }
    - {
        src: "{{ certificate_dirs.base_path }}/apiserver.pem",
        dest: "{{ kubernetes_pem }}",
        mode: "0644",
      }
    - {
        src: "{{ certificate_dirs.base_path }}/apiserver-key.pem",
        dest: "{{ kubernetes_key_pem }}",
        mode: "0600",
      }
    - {
        src: "{{ certificate_dirs.base_path }}/admin.pem",
        dest: "{{ admin_pem }}",
        mode: "0644",
      }
    - {
        src: "{{ certificate_dirs.base_path }}/admin-key.pem",
        dest: "{{ admin_key_pem }}",
        mode: "0600",
      }
    - {
        src: "{{ certificate_dirs.base_path }}/sa.pub",
        dest: "{{ service_account_pem }}",
        mode: "0644",
      }
    - {
        src: "{{ certificate_dirs.base_path }}/sa-key.pem",
        dest: "{{ service_account_key_pem }}",
        mode: "0600",
      }

- name: Copy control plane certificates to master nodes
  ansible.builtin.copy:
    src: "{{ item.src }}"
    dest: "{{ item.dest }}"
    mode: "{{ item.mode }}"
    owner: root
    group: root
  loop:
    - {
        src: "{{ certificate_dirs.base_path }}/kube-controller-manager.pem",
        dest: "{{ kube_controller_manager_pem }}",
        mode: "0644",
      }
    - {
        src: "{{ certificate_dirs.base_path }}/kube-controller-manager-key.pem",
        dest: "{{ kube_controller_manager_key_pem }}",
        mode: "0600",
      }
    - {
        src: "{{ certificate_dirs.base_path }}/kube-scheduler.pem",
        dest: "{{ kube_scheduler_pem }}",
        mode: "0644",
      }
    - {
        src: "{{ certificate_dirs.base_path }}/kube-scheduler-key.pem",
        dest: "{{ kube_scheduler_key_pem }}",
        mode: "0600",
      }
  when: inventory_hostname in groups['masters']

- name: Copy kubelet certificates to worker and master nodes
  ansible.builtin.copy:
    src: "{{ certificate_dirs.base_path }}/kubelet.pem"
    dest: "{{ kubelet_pem }}"
    mode: "0644"
    owner: root
    group: root
  when: inventory_hostname in groups['workers'] or inventory_hostname in groups['masters']

- name: Copy kubelet keys to worker and master nodes
  ansible.builtin.copy:
    src: "{{ certificate_dirs.base_path }}/kubelet-key.pem"
    dest: "{{ kubelet_key_pem }}"
    mode: "0600"
    owner: root
    group: root
  when: inventory_hostname in groups['workers'] or inventory_hostname in groups['masters']

- name: Copy kube-proxy certificates to worker nodes
  ansible.builtin.copy:
    src: "{{ certificate_dirs.base_path }}/kube-proxy.pem"
    dest: "{{ pki_dir }}/kube-proxy.pem"
    mode: "0644"
    owner: root
    group: root
  when: inventory_hostname in groups['workers']

- name: Copy kube-proxy keys to worker nodes
  ansible.builtin.copy:
    src: "{{ certificate_dirs.base_path }}/kube-proxy-key.pem"
    dest: "{{ pki_dir }}/kube-proxy-key.pem"
    mode: "0600"
    owner: root
    group: root
  when: inventory_hostname in groups['workers']

- name: Copy etcd certificates to etcd directory on master nodes
  ansible.builtin.copy:
    src: "{{ item.src }}"
    dest: "{{ item.dest }}"
    mode: "{{ item.mode }}"
    owner: root
    group: root
  loop:
    - {
        src: "{{ certificate_dirs.base_path }}/etcd-server.pem",
        dest: "{{ etcd_pem }}",
        mode: "0644",
      }
    - {
        src: "{{ certificate_dirs.base_path }}/etcd-server-key.pem",
        dest: "{{ etcd_key_pem }}",
        mode: "0600",
      }
  when: inventory_hostname in groups['masters']

- name: Verify CA certificate was copied successfully
  ansible.builtin.stat:
    path: "{{ ca_pem }}"
  register: ca_cert
  failed_when: not ca_cert.stat.exists

- name: Verify CA key was copied successfully
  ansible.builtin.stat:
    path: "{{ ca_key_pem }}"
  register: ca_key
  failed_when: not ca_key.stat.exists

- name: Verify Kubernetes certificate was copied successfully
  ansible.builtin.stat:
    path: "{{ kubernetes_pem }}"
  register: k8s_cert
  failed_when: not k8s_cert.stat.exists


=== ./ansible/roles/jumpbox_setup/meta/main.yml ===

dependencies: []


=== ./ansible/roles/jumpbox_setup/tasks/main.yml ===

- name: Download binaries with correct executable permissions
  ansible.builtin.get_url:
    url: "{{ item.url }}"
    dest: "/home/vagrant/k8s-project/ansible/roles/{{ item.dest_dir }}/files/{{ item.filename }}"
    mode: "0755"
  loop:
    - {
        url: "https://github.com/cloudflare/cfssl/releases/download/v1.6.5/cfssl_1.6.5_linux_amd64",
        dest_dir: "certificates-generate",
        filename: "cfssl",
      }
    - {
        url: "https://github.com/cloudflare/cfssl/releases/download/v1.6.5/cfssljson_1.6.5_linux_amd64",
        dest_dir: "certificates-generate",
        filename: "cfssljson",
      }
    - {
        url: "https://github.com/etcd-io/etcd/releases/download/v3.6.0/etcd-v3.6.0-linux-amd64.tar.gz",
        dest_dir: "etcd",
        filename: "etcd.tar.gz",
        mode: "0644",
      }
    - {
        url: "https://dl.k8s.io/v1.34.1/bin/linux/amd64/kube-apiserver",
        dest_dir: "control-plane",
        filename: "kube-apiserver",
      }
    - {
        url: "https://dl.k8s.io/v1.34.1/bin/linux/amd64/kube-controller-manager",
        dest_dir: "control-plane",
        filename: "kube-controller-manager",
      }
    - {
        url: "https://dl.k8s.io/v1.34.1/bin/linux/amd64/kube-scheduler",
        dest_dir: "control-plane",
        filename: "kube-scheduler",
      }
    - {
        url: "https://dl.k8s.io/v1.34.1/bin/linux/amd64/kubelet",
        dest_dir: "kubelet",
        filename: "kubelet",
      }
    - {
        url: "https://dl.k8s.io/v1.34.1/bin/linux/amd64/kube-proxy",
        dest_dir: "kube-proxy",
        filename: "kube-proxy",
      }
    - {
        url: "https://github.com/opencontainers/runc/releases/download/v1.2.0/runc.amd64",
        dest_dir: "container-runtime",
        filename: "runc",
      }
    - {
        url: "https://github.com/containerd/containerd/releases/download/v2.0.0/containerd-2.0.0-linux-amd64.tar.gz",
        dest_dir: "container-runtime",
        filename: "containerd.tar.gz",
        mode: "0644",
      }
    - {
        url: "https://dl.k8s.io/v1.34.1/bin/linux/amd64/kubectl",
        dest_dir: "networking",
        filename: "kubectl",
      }
    - {
        url: "https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml",
        dest_dir: "cni",
        filename: "flannel.yml",
        mode: "0644",
      }
    - {
        url: "https://docs.projectcalico.org/manifests/calico.yaml",
        dest_dir: "cni",
        filename: "calico.yaml",
        mode: "0644",
      }
  when: inventory_hostname == 'jumpbox'


=== ./ansible/roles/networking/defaults/main.yml ===

# Networking paths
coredns_manifest: "{{ config_dir }}/coredns-deployment.yaml"
kubectl_binary_dest: /usr/local/bin/kubectl


=== ./ansible/roles/networking/templates/coredns-deployment.yaml.j2 ===

apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
spec:
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  selector:
    k8s-app: kube-dns
  clusterIP: "{{ service_cidr | ipaddr('10') }}"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
spec:
  replicas: 2
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      containers:
      - name: coredns
        image: coredns/coredns:1.11.3
        args:
        - -conf
        - /etc/coredns/Corefile
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
      volumes:
      - name: config-volume
        configMap:
          name: coredns
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local {{ pod_network_cidr | ipaddr('net') }}
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
    }

=== ./ansible/roles/networking/meta/main.yml ===

dependencies: []


=== ./ansible/roles/networking/handlers/main.yml ===

- name: Apply CoreDNS
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} apply -f {{ coredns_manifest }}"
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/roles/networking/tasks/main.yml ===

- name: Deploy CoreDNS
  include_tasks: deploy_coredns.yml


=== ./ansible/roles/networking/tasks/deploy_coredns.yml ===

- name: Copy kubectl binary
  ansible.builtin.copy:
    src: "{{ kubectl_binary }}"
    dest: "{{ kubectl_binary}}"
    mode: "0755"

- name: Apply CoreDNS manifest
  ansible.builtin.template:
    src: coredns-deployment.yaml.j2
    dest: "{{ coredns_manifest }}"
    owner: root
    group: root
    mode: "0644"
  notify: apply_coredns
  when: inventory_hostname == 'master1'

- name: Apply CoreDNS configuration
  ansible.builtin.command:
    cmd: "{{ kubectl_binary}} apply -f {{ coredns_manifest }}"
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/ansible.cfg ===

[defaults]
inventory = inventory/hosts.ini
remote_user = vagrant
roles_path = roles
private_key_file = /home/vagrant/.ssh/id_rsa
host_key_checking = False
retry_files_enabled = False
stdout_callback = yaml

[privilege_escalation]
become = True
become_method = sudo
become_user = root
become_ask_pass = False

=== ./provision_jumpbox.sh ===

#!/bin/bash

set -e  # Exit on error

# Function for formatted task output
task_echo() {
    echo "===> $1"
}

task_echo "[Task 1] - Install required packages"
{
    apt-get update
    apt-get install -y ansible
}

task_echo "[Task 2] - Verify directory structure"
{
    echo "Current directory: $(pwd)"
    echo "Listing /home/vagrant/k8s-project:"
    ls -la /home/vagrant/k8s-project/
    echo "Checking ansible directory:"
    ls -la /home/vagrant/k8s-project/ansible/ || echo "Ansible directory not found!"
}

task_echo "[Task 3] - Run Ansible playbook for jumpbox setup"
{
    # Check if ansible directory exists
    if [ -d "/home/vagrant/k8s-project/ansible" ]; then
        cd /home/vagrant/k8s-project/ansible
        ansible-playbook -i inventory/hosts.ini playbooks/jumpbox_setup.yml --connection=local -l jumpbox
    else
        echo "ERROR: Ansible directory not found at /home/vagrant/k8s-project/ansible/"
        echo "Available content in /home/vagrant/k8s-project/:"
        ls -la /home/vagrant/k8s-project/
        exit 1
    fi
}

echo "Jumpbox provisioning complete."

=== ./scripts/check_requirements.sh ===

#!/bin/bash

# Define cluster resource requirements for 1 master, 2 workers, 1 jumpbox
REQ_CPU_CORES=6           # 1 master (2 CPUs) + 2 workers (1 CPU each) + 1 jumpbox (1 CPU) + 1 buffer
REQ_RAM_MB=7168           # 1 master (3072MB) + 2 workers (2048MB each) + 1 jumpbox (512MB) + 10% buffer
REQ_DISK_GB=60            # 1 master (20GB) + 2 workers (20GB each) + 1 jumpbox (10GB) + 10% buffer

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
NC='\033[0m' # No Color

echo "Checking system requirements for 1-master, 2-worker, 1-jumpbox Kubernetes cluster..."

# Initialize variables for status tracking
CPU_STATUS="PASS"
RAM_STATUS="PASS"
DISK_STATUS="PASS"

# 1. CPU Check
# Install sysstat if not present (for mpstat)
if ! command -v mpstat &> /dev/null; then
    echo "Installing sysstat for mpstat..."
    sudo apt-get update && sudo apt-get install -y sysstat
fi

TOTAL_CPU_CORES=$(nproc)
# Get idle percentage from mpstat (single snapshot)
IDLE_PERCENT=$(mpstat 1 1 | grep -A 1 "all" | tail -1 | awk '{print $NF}')
USED_PERCENT=$(echo "100 - $IDLE_PERCENT" | bc -l)
USED_CPU_CORES=$(echo "$USED_PERCENT * $TOTAL_CPU_CORES / 100" | bc -l | awk '{printf "%.1f", $0}')
AVAILABLE_CPU_CORES=$(echo "$IDLE_PERCENT * $TOTAL_CPU_CORES / 100" | bc -l | awk '{printf "%.1f", $0}')
echo "CPU Check:"
echo "  Total Capacity: ${TOTAL_CPU_CORES} cores"
echo "  Used: ${USED_CPU_CORES} cores (${USED_PERCENT}%)"
echo "  Available: ${AVAILABLE_CPU_CORES} cores (${IDLE_PERCENT}%)"
echo "  Required: ${REQ_CPU_CORES} cores"
if [ "$(echo "$AVAILABLE_CPU_CORES >= $REQ_CPU_CORES" | bc -l)" -eq 1 ]; then
    echo -e "  Status: ${GREEN}PASS${NC}"
else
    echo -e "  Status: ${RED}FAIL - Need at least ${REQ_CPU_CORES} cores available${NC}"
    CPU_STATUS="FAIL"
fi
echo ""

# 2. RAM Check
TOTAL_RAM_MB=$(free -m | awk '/Mem:/ {print $2}')
USED_RAM_MB=$(free -m | awk '/Mem:/ {print $3}')
AVAILABLE_RAM_MB=$(free -m | awk '/Mem:/ {print $4}')
echo "RAM Check:"
echo "  Total Capacity: ${TOTAL_RAM_MB}MB"
echo "  Used: ${USED_RAM_MB}MB"
echo "  Available: ${AVAILABLE_RAM_MB}MB"
echo "  Required: ${REQ_RAM_MB}MB"
if [ "$AVAILABLE_RAM_MB" -ge "$REQ_RAM_MB" ]; then
    echo -e "  Status: ${GREEN}PASS${NC}"
else
    echo -e "  Status: ${RED}FAIL - Need at least ${REQ_RAM_MB}MB free RAM${NC}"
    RAM_STATUS="FAIL"
fi
echo ""

# 3. Disk Check
DISK_PATH="$HOME/VirtualBox VMs"
if [ ! -d "$DISK_PATH" ]; then
    DISK_PATH="$HOME"
fi
TOTAL_DISK_GB=$(df -h "$DISK_PATH" | tail -1 | awk '{print int($2)}')
USED_DISK_GB=$(df -h "$DISK_PATH" | tail -1 | awk '{print int($3)}')
AVAILABLE_DISK_GB=$(df -h "$DISK_PATH" | tail -1 | awk '{print int($4)}')
echo "Disk Check (at $DISK_PATH):"
echo "  Total Capacity: ${TOTAL_DISK_GB}GB"
echo "  Used: ${USED_DISK_GB}GB"
echo "  Available: ${AVAILABLE_DISK_GB}GB"
echo "  Required: ${REQ_DISK_GB}GB"
if [ "$AVAILABLE_DISK_GB" -ge "$REQ_DISK_GB" ]; then
    echo -e "  Status: ${GREEN}PASS${NC}"
else
    echo -e "  Status: ${RED}FAIL - Need at least ${REQ_DISK_GB}GB free disk space${NC}"
    DISK_STATUS="FAIL"
fi
echo ""

# Summary
echo "Summary of Requirements Check:"
echo -e "  CPU: ${CPU_STATUS}"
echo -e "  RAM: ${RAM_STATUS}"
echo -e "  Disk: ${DISK_STATUS}"
echo ""

if [ "$CPU_STATUS" = "PASS" ] && [ "$RAM_STATUS" = "PASS" ] && [ "$DISK_STATUS" = "PASS" ]; then
    echo -e "${GREEN}All checks passed! Your system meets the requirements for the Kubernetes cluster setup.${NC}"
else
    echo -e "${RED}One or more checks failed. Please ensure your system meets the minimum requirements before proceeding.${NC}"
    exit 1
fi


=== ./scripts/distribute-ssh-keys.sh ===

#!/bin/bash

set -e

echo "=== Setting up SSH keys for cluster access ==="

# Use the mounted Vagrant insecure keys
VAGRANT_KEY="/home/vagrant/vagrant-keys/vagrant.key.rsa"

if [ ! -f "$VAGRANT_KEY" ]; then
    echo "ERROR: Vagrant RSA key not found at $VAGRANT_KEY"
    exit 1
fi

echo "Using Vagrant key: $VAGRANT_KEY"

# Generate new SSH key pair on jumpbox
echo "Generating new SSH key pair..."
sudo -u vagrant mkdir -p /home/vagrant/.ssh
sudo -u vagrant ssh-keygen -t rsa -b 4096 -f /home/vagrant/.ssh/id_rsa -N "" -q
sudo -u vagrant chmod 600 /home/vagrant/.ssh/id_rsa

# Read the public key
PUBLIC_KEY=$(sudo -u vagrant cat /home/vagrant/.ssh/id_rsa.pub)

# Distribute the public key to all nodes using Vagrant's insecure key
for IP in 192.168.56.11 192.168.56.21 192.168.56.22; do
    echo "Distributing key to $IP..."
    
    # Use Vagrant's key to SSH into each node and add our public key
    ssh -i "$VAGRANT_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null vagrant@$IP "
        mkdir -p /home/vagrant/.ssh
        echo '$PUBLIC_KEY' >> /home/vagrant/.ssh/authorized_keys
        chmod 700 /home/vagrant/.ssh
        chmod 600 /home/vagrant/.ssh/authorized_keys
        echo 'Key deployed to $IP'
    "
done

# Also add the key to jumpbox itself
echo "$PUBLIC_KEY" >> /home/vagrant/.ssh/authorized_keys
chmod 600 /home/vagrant/.ssh/authorized_keys

echo "=== SSH setup complete ==="
echo "New SSH key generated and distributed to all nodes"

