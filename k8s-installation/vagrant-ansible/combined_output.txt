=== ./.gitignore ===

files/

=== ./provision_common.sh ===

#!/bin/bash

set -e

echo "Running common provisioning tasks..."

# Add host entries to ALL nodes (jumpbox, masters, workers)
cat >> /etc/hosts << 'EOF'
# Kubernetes cluster nodes
192.168.56.40 jumpbox
192.168.56.11 master1
192.168.56.21 worker1
192.168.56.22 worker2
EOF

# Disable swap
swapoff -a
sed -i '/ swap / s/^/#/' /etc/fstab

# Install and configure NTP
apt-get update
apt-get install -y chrony curl wget git net-tools
systemctl enable chrony
systemctl start chrony

echo "Common provisioning complete."

=== ./Vagrantfile ===

# -*- mode: ruby -*-
# vi: set ft=ruby :

require 'yaml'

# Load network configuration from network.yml
network_config = YAML.load_file('ansible/inventory/group_vars/network.yml')

# Define resources
RESOURCES = {
  "master" => { "ram" => 2048, "cpu" => 2, "disk" => "50GB" },
  "worker" => { "ram" => 2048, "cpu" => 1, "disk" => "30GB" },
}
JUMP_RAM = 1024
JUMP_DISK = "20GB"

Vagrant.configure("2") do |config|
  config.vm.box = "ubuntu/jammy64"
  config.ssh.insert_key = false

  # Master nodes configuration
  network_config['network']['masters'].each_with_index do |master, index|
    config.vm.define master['name'] do |master_node|
      master_node.vm.network "private_network", ip: master['ip']
      master_node.vm.network "forwarded_port", guest: 22, host: 2711 + index, id: "ssh"
      master_node.vm.hostname = master['name']
      master_node.vm.provider "virtualbox" do |vb|
        vb.memory = RESOURCES["master"]["ram"]
        vb.cpus = RESOURCES["master"]["cpu"]
      end
      master_node.vm.provision "shell", path: "provision_common.sh"
    end
  end

  # Worker nodes configuration
  network_config['network']['workers'].each_with_index do |worker, index|
    config.vm.define worker['name'] do |worker_node|
      worker_node.vm.network "private_network", ip: worker['ip']
      worker_node.vm.network "forwarded_port", guest: 22, host: 2721 + index, id: "ssh"
      worker_node.vm.hostname = worker['name']
      worker_node.vm.provider "virtualbox" do |vb|
        vb.memory = RESOURCES["worker"]["ram"]
        vb.cpus = RESOURCES["worker"]["cpu"]
      end
      worker_node.vm.provision "shell", path: "provision_common.sh"
    end
  end

  # Jumpbox configuration (provision LAST)
  config.vm.define "jumpbox" do |jumpbox|
    jumpbox.vm.network "private_network", ip: network_config['network']['jumpbox']['ip']
    jumpbox.vm.network "forwarded_port", guest: 22, host: 2731, id: "ssh"
    jumpbox.vm.hostname = network_config['network']['jumpbox']['name']
    jumpbox.vm.provider "virtualbox" do |vb|
      vb.memory = JUMP_RAM
      vb.cpus = 1
    end

    config.vm.synced_folder ".", "/home/vagrant/k8s-project"
    config.vm.synced_folder "#{Dir.home}/.vagrant.d/insecure_private_keys", "/home/vagrant/vagrant-keys"
    
    # Run provisioning on jumpbox too
    jumpbox.vm.provision "shell", path: "provision_common.sh"
    jumpbox.vm.provision "shell", path: "provision_jumpbox.sh"
    jumpbox.vm.provision "shell", path: "scripts/distribute-ssh-keys.sh"
  end

  # Trigger for cluster provisioning (runs after jumpbox is fully provisioned)
  config.trigger.after :up do |trigger|
    trigger.name = "provision-cluster"
    trigger.info = "Running Ansible to provision Kubernetes cluster"
    trigger.run_remote = {
      inline: "cd /home/vagrant/k8s-project/ansible && ansible-playbook -i inventory/hosts.ini playbooks/cluster.yml"
    }
    trigger.only_on = "jumpbox"
  end

end


=== ./combine.sh ===

#!/bin/bash

# Script to combine all files into a single file with path headers
# Usage: ./combine_files.sh [output_file]

OUTPUT_FILE="${1:-combined_output.txt}"

# Function to process each file
process_file() {
    local file="$1"
    echo "=== $file ==="
    echo ""
    cat "$file"
    echo ""
    echo ""
}

# Export the function so it can be used by find
export -f process_file

# Clear the output file if it exists
> "$OUTPUT_FILE"

# Build find command with exclusions
find . \
  -type d \( -name '.vscode' -o -name '.vagrant' -o -name '.qodo' -o -name 'files' \) -prune -o \
  -type f \
  ! -name "$OUTPUT_FILE" \
  ! -name "ansible-tree.md" \
  ! -iname "readme.md" \
  -exec bash -c 'process_file "$0"' {} \; >> "$OUTPUT_FILE"

echo "All files have been combined into: $OUTPUT_FILE"
echo "Total files processed: $(find . \
  -type d \( -name '.vscode' -o -name '.vagrant' -o -name '.qodo' -o -name 'files' \) -prune -o \
  -type f \
  ! -name "$OUTPUT_FILE" \
  ! -name "ansible-tree.md" \
  ! -iname "readme.md" \
  -print | wc -l)"

=== ./hosts.txt ===

192.168.56.11 master1
192.168.56.21 worker1
192.168.56.22 worker2
192.168.56.40 jumpbox

=== ./ansible/inventory/group_vars/paths.yml ===

paths:
  artifacts:
    base: "/home/vagrant/k8s-project/ansible/artifacts"
    binaries: "{{ paths.artifacts.base }}/binaries"
    certs: "{{ paths.artifacts.base }}/certs"
    kubeconfigs: "{{ paths.artifacts.base }}/kubeconfigs"
    manifests: "{{ paths.artifacts.base }}/manifests"
  kubernetes:
    pki_dir: "{{ pki_dir }}"
    config_dir: "{{ config_dir }}"
    manifests_dir: "{{ manifests_dir }}"
  etcd:
    cert_dir: "{{ certificate_dirs.etcd }}"
    data_dir: "{{ etcd_data_dir }}"


=== ./ansible/inventory/group_vars/workers.yml ===

# Kubelet settings
kubelet_cert_file: "{{ inventory_hostname }}.pem"
kubelet_key_file: "kubelet-key.pem"
kube_proxy_cert_file: "kube-proxy.pem"
kube_proxy_key_file: "kube-proxy-key.pem"

kubelet_args:
  - --config={{ kubernetes.config_dir }}/kubelet-config.yaml
  - --kubeconfig={{ kubernetes.config_dir }}/kubelet.kubeconfig
  - --tls-cert-file={{ kubernetes.pki_dir }}/{{ kubelet_cert_file }}
  - --tls-private-key-file={{ kubernetes.pki_dir }}/{{ kubelet_key_file }}

kube_proxy_args:
  - --config={{ kubernetes.config_dir }}/kube-proxy-config.yaml


=== ./ansible/inventory/group_vars/all.yml ===

# Global paths
base_dir: /etc/kubernetes
pki_dir: /var/lib/kubernetes
manifests_dir: "{{ base_dir }}/manifests"
config_dir: "{{ base_dir }}/configs"

# Certificate destination directories
certificate_dirs:
  kubernetes: "{{ pki_dir }}"
  etcd: /etc/etcd
  kubelet: "{{ pki_dir }}"
  kube_proxy: "{{ pki_dir }}"

# etcd settings
etcd_data_dir: /var/lib/etcd
etcd_cert_dir: "{{ certificate_dirs.etcd }}"

# Binary paths
etcd_binary: "{{ paths.artifacts.binaries }}/etcd"
etcdctl_binary: "{{ paths.artifacts.binaries }}/etcdctl"
kube_apiserver_binary: "{{ paths.artifacts.binaries }}/kube-apiserver"
kube_controller_manager_binary: "{{ paths.artifacts.binaries }}/kube-controller-manager"
kube_scheduler_binary: "{{ paths.artifacts.binaries }}/kube-scheduler"
kubelet_binary: "{{ paths.artifacts.binaries }}/kubelet"
kube_proxy_binary: "{{ paths.artifacts.binaries }}/kube-proxy"
runc_binary: "{{ paths.artifacts.binaries }}/runc"
containerd_binary: "{{ paths.artifacts.binaries }}/containerd"
kubectl_binary: "{{ paths.artifacts.binaries }}/kubectl"
flannel_manifest: "{{ paths.artifacts.manifests }}/kube-flannel.yml"

# Kubernetes versions and settings
kubernetes_version: 1.34.1
etcd_version: 3.6.0
containerd_version: 2.0.0

# Kubernetes API server addresses
kubernetes_public_address: "{{ network.masters[0].ip }}"
kubernetes_internal_address: 10.32.0.1

# Network settings
service_cidr: 10.96.0.0/12
api_server_port: 6443
api_server_endpoint: https://{{ kubernetes_public_address }}:6443
cluster_name: kubernetes

# System settings
swap_enabled: false
ntp_package: chrony

# Certificate settings
cert_key_size: 2048
cert_expiry: "8760h"

ssh_public_key_path: /home/vagrant/k8s-project/.vagrant/machines/jumpbox/virtualbox/private_key.pub
common_firewall_ports:
  - 22/tcp
  - 6443/tcp
  - 2379/tcp
  - 2380/tcp

cni_plugin: flannel
cni_manifest_paths:
  flannel: "{{ paths.artifacts.manifests }}/kube-flannel.yml"
  calico: "{{ paths.artifacts.manifests }}/calico.yaml"

cni_configs:
  flannel:
    pod_network_cidr: 10.244.0.0/16
  calico:
    pod_network_cidr: 192.168.0.0/16

pod_network_cidr: "{{ cni_configs[cni_plugin].pod_network_cidr }}"

cni_extra_ports:
  flannel:
    - 8472/udp
  calico:
    - 179/tcp
    - 4789/udp


=== ./ansible/inventory/group_vars/masters.yml ===

# Control plane settings
kube_apiserver_bind_address: 0.0.0.0
kube_apiserver_port: "{{ api_server_port }}"
kube_apiserver_args:
  - --etcd-servers=https://{{ kubernetes_public_address }}:2379
  - --etcd-cafile={{ kubernetes.pki_dir }}/ca.pem
  - --etcd-certfile={{ kubernetes.pki_dir }}/apiserver-etcd-client.pem
  - --etcd-keyfile={{ kubernetes.pki_dir }}/apiserver-etcd-client-key.pem
  - --client-ca-file={{ kubernetes.pki_dir }}/ca.pem
  - --service-account-key-file={{ kubernetes.pki_dir }}/sa.pub
  - --service-account-signing-key-file={{ kubernetes.pki_dir }}/sa-key.pem
  - --service-account-issuer=https://kubernetes.default.svc.cluster.local
  - --tls-cert-file={{ kubernetes.pki_dir }}/apiserver.pem
  - --tls-private-key-file={{ kubernetes.pki_dir }}/apiserver-key.pem
  - --service-cluster-ip-range={{ service_cidr }}
  - --encryption-provider-config={{ kubernetes.config_dir }}/encryption-config.yaml

kube_controller_manager_args:
  - --cluster-signing-cert-file={{ kubernetes.pki_dir }}/ca.pem
  - --cluster-signing-key-file={{ kubernetes.pki_dir }}/ca-key.pem
  - --service-account-private-key-file={{ kubernetes.pki_dir }}/sa-key.pem
  - --root-ca-file={{ kubernetes.pki_dir }}/ca.pem

kube_scheduler_args:
  - --authentication-kubeconfig={{ kubernetes.config_dir }}/kube-scheduler.kubeconfig
  - --authorization-kubeconfig={{ kubernetes.config_dir }}/kube-scheduler.kubeconfig


=== ./ansible/inventory/group_vars/versions.yml ===

versions:
  kubernetes: "1.34.1"
  etcd: "3.6.4"
  containerd: "2.1.4"
  runc: "1.3.1"
  flannel: "master"
  calico: "3.27.3"
download_urls:
  kubernetes:
    base: "https://dl.k8s.io/{{ versions.kubernetes }}/bin/linux/amd64"
    apiserver: "{{ download_urls.kubernetes.base }}/kube-apiserver"
    controller_manager: "{{ download_urls.kubernetes.base }}/kube-controller-manager"
    scheduler: "{{ download_urls.kubernetes.base }}/kube-scheduler"
    kubelet: "{{ download_urls.kubernetes.base }}/kubelet"
    kube_proxy: "{{ download_urls.kubernetes.base }}/kube-proxy"
    kubectl: "{{ download_urls.kubernetes.base }}/kubectl"
  etcd:
    tarball: "https://github.com/etcd-io/etcd/releases/download/v{{ versions.etcd }}/etcd-v{{ versions.etcd }}-linux-amd64.tar.gz"
  containerd:
    tarball: "https://github.com/containerd/containerd/releases/download/v{{ versions.containerd }}/containerd-{{ versions.containerd }}-linux-amd64.tar.gz"
  runc:
    binary: "https://github.com/opencontainers/runc/releases/download/v{{ versions.runc }}/runc.amd64"
  manifests:
    flannel: "https://raw.githubusercontent.com/flannel-io/flannel/{{ versions.flannel }}/Documentation/kube-flannel.yml"
    calico: "https://raw.githubusercontent.com/projectcalico/calico/v{{ versions.calico }}/manifests/calico.yaml"
checksums:
  kubernetes:
    apiserver: "a170577ea20f8c37a522114a259c36ec4ed397482d8f282c265248dedf2216c3"
    controller_manager: "91b7aa8c599f93515f347d3ee7fb12229a40db0ca02da3e7c5dc96fbca3ad2f3"
    scheduler: "460f5bc48ef176c82fea3592f66ba71ae5bda376f93d86f7fe44fc4759ac577b"
    kubelet: "5a72c596c253ea0b0e5bcc6f29903fd41d1d542a7cadf3700c165a2a041a8d82"
    kube_proxy: "325e48cf35029a382e7180bc0912be8737712b7a4c695948ac2e50041998aebc"
    kubectl: "6a66bc08d6c637fcea50c19063cf49e708fde1630a7f1d4ceca069a45a87e6f1"
  etcd:
    tarball: "sha256:4d5f3101daa534e45ccaf3eec8d21c19b7222db377bcfd5e5a9144155238c105"
  containerd:
    tarball: "sha256:316d510a0428276d931023f72c09fdff1a6ba81d6cc36f31805fea6a3c88f515"
  runc:
    binary: "sha256:53bfce31ca047e537e0767b21c9d529d4b5b3e1cb9c590ca81654f9a5615d80d"


=== ./ansible/inventory/group_vars/network.yml ===

network:
  masters:
    - ip: 192.168.56.11
      name: master1
  workers:
    - ip: 192.168.56.21
      name: worker1
    - ip: 192.168.56.22
      name: worker2
  jumpbox:
    ip: 192.168.56.40
    name: jumpbox
  pod_cidr: "{{ pod_network_cidr }}"
  service_cidr: 10.96.0.0/12
  dns_service_ip: "{{ service_cidr | ipaddr('10') }}"


=== ./ansible/inventory/group_vars/etcd.yml ===

etcd_args:
  - --name={{ inventory_hostname }}
  - --data-dir={{ etcd.data_dir }}
  - --client-cert-auth=true
  - --cert-file={{ etcd.cert_dir }}/etcd-server.pem
  - --key-file={{ etcd.cert_dir }}/etcd-server-key.pem
  - --trusted-ca-file={{ kubernetes.pki_dir }}/ca.pem
  - --peer-cert-file={{ etcd.cert_dir }}/etcd-server.pem
  - --peer-key-file={{ etcd.cert_dir }}/etcd-server-key.pem
  - --peer-trusted-ca-file={{ kubernetes.pki_dir }}/ca.pem
  - --initial-cluster={{ etcd_initial_cluster }}
  - --initial-cluster-state=new
  - --listen-client-urls=https://0.0.0.0:2379
  - --advertise-client-urls=https://{{ kubernetes_public_address }}:2379
  - --listen-peer-urls=https://0.0.0.0:2380
  - --initial-advertise-peer-urls=https://{{ kubernetes_public_address }}:2380
etcd_initial_cluster: "{{ groups.etcd | map('regex_replace', '^(.*)$', '\\1=https://' ~ hostvars[item].ansible_host ~ ':2380') | join(',') }}"


=== ./ansible/inventory/host_vars/worker1.yml ===

node_name: worker1
kubelet_kubeconfig: "{{ kubernetes.config_dir }}/kubelet.kubeconfig"
kube_proxy_kubeconfig: "{{ kubernetes.config_dir }}/kube-proxy.kubeconfig"


=== ./ansible/inventory/host_vars/worker2.yml ===

node_name: worker2
kubelet_kubeconfig: "{{ kubernetes.config_dir }}/kubelet.kubeconfig"
kube_proxy_kubeconfig: "{{ kubernetes.config_dir }}/kube-proxy.kubeconfig"


=== ./ansible/inventory/host_vars/master1.yml ===

node_name: master1
api_server_endpoint: https://{{ kubernetes_public_address }}:6443
kube_apiserver_kubeconfig: "{{ kubernetes.config_dir }}/kube-apiserver.kubeconfig"
kube_controller_manager_kubeconfig: "{{ kubernetes.config_dir }}/kube-controller-manager.kubeconfig"
kube_scheduler_kubeconfig: "{{ kubernetes.config_dir }}/kube-scheduler.kubeconfig"


=== ./ansible/inventory/hosts.ini ===

[masters]
master1 ansible_host=192.168.56.11 ansible_user=vagrant

[etcd]
master1

[workers]
worker1 ansible_host=192.168.56.21 ansible_user=vagrant
worker2 ansible_host=192.168.56.22 ansible_user=vagrant

[jumpboxes]
jumpbox ansible_host=192.168.56.40 ansible_user=vagrant

[all:vars]
ansible_ssh_common_args='-o StrictHostKeyChecking=no'

=== ./ansible/playbooks/kube-proxy.yml ===

- name: Configure kube-proxy
  hosts: workers
  become: yes
  roles:
    - kube-proxy


=== ./ansible/playbooks/etcd-cluster.yml ===

- name: Configure etcd cluster
  hosts: etcd
  become: yes
  roles:
    - etcd


=== ./ansible/playbooks/hardening.yml ===

- name: Apply security hardening
  hosts: masters:etcd
  become: yes
  roles:
    - hardening


=== ./ansible/playbooks/distribute-certificates.yml ===

- name: Distribute certificates to all nodes
  hosts: all:!jumpboxes
  become: yes
  roles:
    - certificates-distribute


=== ./ansible/playbooks/control-plane.yml ===

- name: Configure Kubernetes control plane
  hosts: masters
  become: yes
  roles:
    - control-plane


=== ./ansible/playbooks/prerequisites.yml ===

- name: Apply prerequisites to all nodes
  hosts: masters:workers
  become: yes
  roles:
    - common


=== ./ansible/playbooks/jumpbox_setup.yml ===

- name: Setup jumpbox with binaries
  connection: local
  hosts: jumpboxes
  become: yes
  roles:
    - jumpbox_setup


=== ./ansible/playbooks/networking.yml ===

- name: Deploy CoreDNS
  hosts: masters
  become: yes
  roles:
    - networking


=== ./ansible/playbooks/cni.yml ===

- name: Configure Flannel CNI
  hosts: masters
  become: yes
  roles:
    - cni


=== ./ansible/playbooks/post-setup.yml ===

- name: Post-provisioning setup tasks
  hosts: jumpboxes
  become: yes
  roles:
    - post_setup


=== ./ansible/playbooks/cluster.yml ===

- name: Setup prerequisites
  import_playbook: prerequisites.yml

- name: Generate certificates
  import_playbook: generate-certificates.yml

- name: Distribute certificates
  import_playbook: distribute-certificates.yml

- name: Setup etcd cluster
  import_playbook: etcd-cluster.yml

- name: Setup control plane
  import_playbook: control-plane.yml

- name: Setup container runtime
  import_playbook: container-runtime.yml

- name: Bootstrap cluster
  import_playbook: bootstrap.yml

- name: Setup kubelet
  import_playbook: kubelet.yml

- name: Setup kube-proxy
  import_playbook: kube-proxy.yml

- name: Setup CNI
  import_playbook: cni.yml

- name: Setup networking
  import_playbook: networking.yml

- name: Harden cluster
  import_playbook: hardening.yml

- name: Post-setup configuration
  import_playbook: post-setup.yml


=== ./ansible/playbooks/bootstrap.yml ===

- name: Configure bootstrap
  hosts: masters
  become: yes
  roles:
    - bootstrap


=== ./ansible/playbooks/generate-certificates.yml ===

- name: Generate certificates on jumpbox
  hosts: jumpboxes
  become: yes
  roles:
    - certificates-generate
  tags: generate-certs


=== ./ansible/playbooks/container-runtime.yml ===

- name: Configure container runtime
  hosts: masters:workers
  become: yes
  roles:
    - container-runtime


=== ./ansible/playbooks/kubelet.yml ===

- name: Configure kubelet
  hosts: workers
  become: yes
  roles:
    - kubelet


=== ./ansible/roles/generate-artifacts/defaults/main.yml ===



=== ./ansible/roles/generate-artifacts/templates/kube-proxy.kubeconfig.j2 ===

apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: {{ paths.artifacts.certs }}/ca.pem
    server: https://{{ kubernetes_public_address }}:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kube-proxy
  name: kube-proxy@kubernetes
current-context: kube-proxy@kubernetes
users:
- name: kube-proxy
  user:
    client-certificate: {{ paths.artifacts.certs }}/kube-proxy.pem
    client-key: {{ paths.artifacts.certs }}/kube-proxy-key.pem

=== ./ansible/roles/generate-artifacts/templates/admin.kubeconfig.j2 ===

apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority-data: {{ lookup('file', paths.artifacts.certs ~ '/ca.pem') | b64encode }}
    server: https://{{ kubernetes_public_address }}:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
users:
- name: kubernetes-admin
  user:
    client-certificate-data: {{ lookup('file', paths.artifacts.certs ~ '/admin.pem') | b64encode }}
    client-key-data: {{ lookup('file', paths.artifacts.certs ~ '/admin-key.pem') | b64encode }}

=== ./ansible/roles/generate-artifacts/templates/kubelet.kubeconfig.j2 ===

apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: {{ paths.artifacts.certs }}/ca.pem
    server: https://{{ kubernetes_public_address }}:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubelet
  name: kubelet@kubernetes
current-context: kubelet@kubernetes
users:
- name: kubelet
  user:
    client-certificate: {{ paths.artifacts.certs }}/kubelet.pem
    client-key: {{ paths.artifacts.certs }}/kubelet-key.pem

=== ./ansible/roles/generate-artifacts/meta/main.yml ===

dependencies:
  - download-artifacts # Requires binaries if any, but mainly paths
galaxy_info:
  role_name: generate-artifacts
  author: David Essien
  description: Generate certificates and kubeconfigs for Kubernetes cluster
  license: MIT


=== ./ansible/roles/generate-artifacts/tasks/main.yml ===

- name: Generate certificates
  include_tasks: generate-certs.yml

- name: Generate kubeconfigs
  include_tasks: generate-kubeconfigs.yml


=== ./ansible/roles/generate-artifacts/tasks/generate-certs.yml ===

- name: Ensure certs directory exists
  file:
    path: "{{ paths.artifacts.certs }}"
    state: directory
    owner: vagrant
    group: vagrant
    mode: '0700'
  delegate_to: localhost

- name: Generate CA private key
  community.crypto.openssl_privatekey:
    path: "{{ paths.artifacts.certs }}/ca-key.pem"
    size: "{{ cert_key_size }}"
    state: present

- name: Generate CA CSR
  community.crypto.openssl_csr:
    path: "{{ paths.artifacts.certs }}/ca.csr"
    privatekey_path: "{{ paths.artifacts.certs }}/ca-key.pem"
    common_name: "Kubernetes"
    basic_constraints:
      - CA: true
    key_usage:
      - keyCertSign
      - cRLSign
    state: present

- name: Self-sign CA certificate
  community.crypto.x509_certificate:
    path: "{{ paths.artifacts.certs }}/ca.pem"
    csr_path: "{{ paths.artifacts.certs }}/ca.csr"
    ownca_path: "{{ paths.artifacts.certs }}/ca.pem"
    ownca_privatekey_path: "{{ paths.artifacts.certs }}/ca-key.pem"
    provider: selfsigned
    state: present

- name: Generate admin private key
  community.crypto.openssl_privatekey:
    path: "{{ paths.artifacts.certs }}/admin-key.pem"
    size: "{{ cert_key_size }}"
    state: present

- name: Generate admin CSR
  community.crypto.openssl_csr:
    path: "{{ paths.artifacts.certs }}/admin.csr"
    privatekey_path: "{{ paths.artifacts.certs }}/admin-key.pem"
    common_name: "admin"
    organization_name: "system:masters"
    organizational_unit_name: "Kubernetes The Hard Way"
    country_name: "US"
    state: "California"
    locality: "San Francisco"
    state: present

- name: Sign admin certificate
  community.crypto.x509_certificate:
    path: "{{ paths.artifacts.certs }}/admin.pem"
    csr_path: "{{ paths.artifacts.certs }}/admin.csr"
    ca_path: "{{ paths.artifacts.certs }}/ca.pem"
    ca_key_path: "{{ paths.artifacts.certs }}/ca-key.pem"
    provider: signcsr
    state: present

- name: Generate API server private key
  community.crypto.openssl_privatekey:
    path: "{{ paths.artifacts.certs }}/apiserver-key.pem"
    size: "{{ cert_key_size }}"
    state: present

- name: Generate API server CSR
  community.crypto.openssl_csr:
    path: "{{ paths.artifacts.certs }}/kubernetes.csr"
    privatekey_path: "{{ paths.artifacts.certs }}/apiserver-key.pem"
    common_name: "kubernetes"
    organization_name: "Kubernetes"
    organizational_unit_name: "Kubernetes"
    country_name: "US"
    state: "Oregon"
    locality: "Portland"
    subject_alt_name:
      - "IP:127.0.0.1"
      - "IP:{{ kubernetes_internal_address }}"
      - "IP:{{ kubernetes_public_address }}"
      - "DNS:kubernetes"
      - "DNS:kubernetes.default"
      - "DNS:kubernetes.default.svc"
      - "DNS:kubernetes.default.svc.cluster.local"
    state: present

- name: Sign API server certificate
  community.crypto.x509_certificate:
    path: "{{ paths.artifacts.certs }}/apiserver.pem"
    csr_path: "{{ paths.artifacts.certs }}/kubernetes.csr"
    ca_path: "{{ paths.artifacts.certs }}/ca.pem"
    ca_key_path: "{{ paths.artifacts.certs }}/ca-key.pem"
    provider: signcsr
    state: present

- name: Generate controller manager private key
  community.crypto.openssl_privatekey:
    path: "{{ paths.artifacts.certs }}/kube-controller-manager-key.pem"
    size: "{{ cert_key_size }}"
    state: present

- name: Generate controller manager CSR
  community.crypto.openssl_csr:
    path: "{{ paths.artifacts.certs }}/kube-controller-manager.csr"
    privatekey_path: "{{ paths.artifacts.certs }}/kube-controller-manager-key.pem"
    common_name: "system:kube-controller-manager"
    organization_name: "system:kube-controller-manager"
    organizational_unit_name: "Kubernetes The Hard Way"
    country_name: "US"
    state: "California"
    locality: "San Francisco"
    state: present

- name: Sign controller manager certificate
  community.crypto.x509_certificate:
    path: "{{ paths.artifacts.certs }}/kube-controller-manager.pem"
    csr_path: "{{ paths.artifacts.certs }}/kube-controller-manager.csr"
    ca_path: "{{ paths.artifacts.certs }}/ca.pem"
    ca_key_path: "{{ paths.artifacts.certs }}/ca-key.pem"
    provider: signcsr
    state: present

- name: Generate scheduler private key
  community.crypto.openssl_privatekey:
    path: "{{ paths.artifacts.certs }}/kube-scheduler-key.pem"
    size: "{{ cert_key_size }}"
    state: present

- name: Generate scheduler CSR
  community.crypto.openssl_csr:
    path: "{{ paths.artifacts.certs }}/kube-scheduler.csr"
    privatekey_path: "{{ paths.artifacts.certs }}/kube-scheduler-key.pem"
    common_name: "system:kube-scheduler"
    organization_name: "system:kube-scheduler"
    organizational_unit_name: "Kubernetes The Hard Way"
    country_name: "US"
    state: "California"
    locality: "San Francisco"
    state: present

- name: Sign scheduler certificate
  community.crypto.x509_certificate:
    path: "{{ paths.artifacts.certs }}/kube-scheduler.pem"
    csr_path: "{{ paths.artifacts.certs }}/kube-scheduler.csr"
    ca_path: "{{ paths.artifacts.certs }}/ca.pem"
    ca_key_path: "{{ paths.artifacts.certs }}/ca-key.pem"
    provider: signcsr
    state: present

- name: Generate service account private key
  community.crypto.openssl_privatekey:
    path: "{{ paths.artifacts.certs }}/sa-key.pem"
    size: "{{ cert_key_size }}"
    state: present

- name: Generate service account public key
  community.crypto.openssl_publickey:
    path: "{{ paths.artifacts.certs }}/sa.pub"
    privatekey_path: "{{ paths.artifacts.certs }}/sa-key.pem"
    state: present

- name: Generate etcd private key
  community.crypto.opensql_privatekey:
    path: "{{ paths.artifacts.certs }}/etcd-server-key.pem"
    size: "{{ cert_key_size }}"
    state: present

- name: Generate etcd CSR
  community.crypto.openssl_csr:
    path: "{{ paths.artifacts.certs }}/etcd.csr"
    privatekey_path: "{{ paths.artifacts.certs }}/etcd-server-key.pem"
    common_name: "etcd"
    organization_name: "etcd"
    organizational_unit_name: "Kubernetes"
    country_name: "US"
    state: "Oregon"
    locality: "Portland"
    subject_alt_name:
      - "IP:127.0.0.1"
      - "IP:{{ kubernetes_public_address }}"
      - "DNS:localhost"
      - "DNS:etcd"
      - "DNS:etcd.local"
    state: present

- name: Sign etcd certificate
  community.crypto.x509_certificate:
    path: "{{ paths.artifacts.certs }}/etcd-server.pem"
    csr_path: "{{ paths.artifacts.certs }}/etcd.csr"
    ca_path: "{{ paths.artifacts.certs }}/ca.pem"
    ca_key_path: "{{ paths.artifacts.certs }}/ca-key.pem"
    provider: signcsr
    state: present

- name: Generate API server etcd client private key
  community.crypto.openssl_privatekey:
    path: "{{ paths.artifacts.certs }}/apiserver-etcd-client-key.pem"
    size: "{{ cert_key_size }}"
    state: present

- name: Generate API server etcd client CSR
  community.crypto.openssl_csr:
    path: "{{ paths.artifacts.certs }}/apiserver-etcd-client.csr"
    privatekey_path: "{{ paths.artifacts.certs }}/apiserver-etcd-client-key.pem"
    common_name: "kube-apiserver-etcd-client"
    organization_name: "system:masters"
    organizational_unit_name: "Kubernetes"
    country_name: "US"
    state: "Oregon"
    locality: "Portland"
    state: present

- name: Sign API server etcd client certificate
  community.crypto.x509_certificate:
    path: "{{ paths.artifacts.certs }}/apiserver-etcd-client.pem"
    csr_path: "{{ paths.artifacts.certs }}/apiserver-etcd-client.csr"
    ca_path: "{{ paths.artifacts.certs }}/ca.pem"
    ca_key_path: "{{ paths.artifacts.certs }}/ca-key.pem"
    provider: signcsr
    state: present

- name: Generate kube-proxy private key
  community.crypto.openssl_privatekey:
    path: "{{ paths.artifacts.certs }}/kube-proxy-key.pem"
    size: "{{ cert_key_size }}"
    state: present

- name: Generate kube-proxy CSR
  community.crypto.openssl_csr:
    path: "{{ paths.artifacts.certs }}/kube-proxy.csr"
    privatekey_path: "{{ paths.artifacts.certs }}/kube-proxy-key.pem"
    common_name: "system:kube-proxy"
    organization_name: "system:node-proxier"
    organizational_unit_name: "Kubernetes The Hard Way"
    country_name: "US"
    state: "Oregon"
    locality: "Portland"
    state: present

- name: Sign kube-proxy certificate
  community.crypto.x509_certificate:
    path: "{{ paths.artifacts.certs }}/kube-proxy.pem"
    csr_path: "{{ paths.artifacts.certs }}/kube-proxy.csr"
    ca_path: "{{ paths.artifacts.certs }}/ca.pem"
    ca_key_path: "{{ paths.artifacts.certs }}/ca-key.pem"
    provider: signcsr
    state: present

- name: Generate kubelet private key (generic, will override per-node)
  community.crypto.openssl_privatekey:
    path: "{{ paths.artifacts.certs }}/kubelet-key.pem"
    size: "{{ cert_key_size }}"
    state: present

- name: Generate kubelet CSR per node
  community.crypto.openssl_csr:
    path: "{{ paths.artifacts.certs }}/{{ item }}.csr"
    privatekey_path: "{{ paths.artifacts.certs }}/kubelet-key.pem"
    common_name: "system:node:{{ item }}"
    organization_name: "system:nodes"
    organizational_unit_name: "Kubernetes The Hard Way"
    country_name: "US"
    state: "California"
    locality: "San Francisco"
    state: present
  loop: "{{ groups['masters'] + groups['workers'] }}"

- name: Sign kubelet certificates per node
  community.crypto.x509_certificate:
    path: "{{ paths.artifacts.certs }}/{{ item }}.pem"
    csr_path: "{{ paths.artifacts.certs }}/{{ item }}.csr"
    ca_path: "{{ paths.artifacts.certs }}/ca.pem"
    ca_key_path: "{{ paths.artifacts.certs }}/ca-key.pem"
    provider: signcsr
    state: present
  loop: "{{ groups['masters'] + groups['workers'] }}"

=== ./ansible/roles/generate-artifacts/tasks/generate-kubeconfigs.yml ===

---
- name: Ensure kubeconfigs directory exists
  file:
    path: "{{ paths.artifacts.kubeconfigs }}"
    state: directory
    owner: vagrant
    group: vagrant
    mode: "0700"
  delegate_to: localhost

- name: Generate admin kubeconfig
  template:
    src: admin.kubeconfig.j2
    dest: "{{ paths.artifacts.kubeconfigs }}/admin.kubeconfig"
    mode: "0600"
    owner: vagrant
    group: vagrant
  delegate_to: localhost

- name: Generate kubelet kubeconfig per node
  template:
    src: kubelet.kubeconfig.j2
    dest: "{{ paths.artifacts.kubeconfigs }}/{{ item }}.kubeconfig"
    mode: "0600"
    owner: vagrant
    group: vagrant
  loop: "{{ groups['masters'] + groups['workers'] }}"
  delegate_to: localhost

- name: Generate kube-proxy kubeconfig
  template:
    src: kube-proxy.kubeconfig.j2
    dest: "{{ paths.artifacts.kubeconfigs }}/kube-proxy.kubeconfig"
    mode: "0600"
    owner: vagrant
    group: vagrant
  delegate_to: localhost


=== ./ansible/roles/post_setup/meta/main.yml ===

dependencies: []


=== ./ansible/roles/post_setup/tasks/main.yml ===

- name: Configure kubectl on jumpbox (post-bootstrap)
  include_tasks: ../jumpbox_setup/tasks/configure_kubectl.yml
  when: inventory_hostname == 'jumpbox'


=== ./ansible/roles/kubelet/defaults/main.yml ===

# Kubelet paths
kubelet_binary_dest: /usr/local/bin/kubelet
kubelet_config: "{{ config_dir }}/kubelet-config.yaml"
kubelet_service: /etc/systemd/system/kubelet.service


=== ./ansible/roles/kubelet/templates/kubelet.service.j2 ===

[Unit]
Description=Kubernetes Kubelet
Documentation=https://kubernetes.io/docs/
After=containerd.service
Requires=containerd.service

[Service]
ExecStart={{ kubelet_binary_dest }} \
    --node-name={{ node_name }} \
    --tls-cert-file={{ pki_dir }}/{{ kubelet_cert_file }} \
    --tls-private-key-file={{ pki_dir }}/{{ kubelet_key_file }} \
    {% for arg in kubelet_args %} {{ arg }} {% endfor %}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

=== ./ansible/roles/kubelet/templates/kubelet-config.yaml.j2 ===

apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
authorization:
  mode: Webhook
clusterDomain: cluster.local
clusterDNS:
  - "{{ service_cidr | ipaddr('10') }}"
tlsCertFile: "{{ pki_dir }}/{{ kubelet_cert_file }}"
tlsPrivateKeyFile: "{{ pki_dir }}/{{ kubelet_key_file }}"
cgroupDriver: systemd


=== ./ansible/roles/kubelet/meta/main.yml ===

dependencies: []


=== ./ansible/roles/kubelet/handlers/main.yml ===

- name: Restart kubelet
  ansible.builtin.systemd:
    name: kubelet
    state: restarted


=== ./ansible/roles/kubelet/tasks/main.yml ===

- name: Configure kubelet
  include_tasks: configure_kubelet.yml


=== ./ansible/roles/kubelet/tasks/configure_kubelet.yml ===

- name: Copy kubelet binary
  ansible.builtin.copy:
    src: "{{ kubelet_binary }}"
    dest: "{{ kubelet_binary_dest }}"
    mode: "0755"

- name: Apply kubelet configuration
  ansible.builtin.template:
    src: kubelet-config.yaml.j2
    dest: "{{ kubelet_config }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kubelet

- name: Apply kubelet service template
  ansible.builtin.template:
    src: kubelet.service.j2
    dest: "{{ kubelet_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kubelet

- name: Ensure kubelet service is enabled and started
  ansible.builtin.systemd:
    name: kubelet
    enabled: yes
    state: started


=== ./ansible/roles/common/defaults/main.yml ===

# Common packages to install
common_packages:
  - curl
  - apt-transport-https
  - ca-certificates

# Firewall ports to allow
common_firewall_ports:
  - 22/tcp # SSH
  - "{{ api_server_port }}/tcp" # Kubernetes API server
  - 2379/tcp # etcd client
  - 2380/tcp # etcd peer
  - 8472/udp # Flannel VXLAN


=== ./ansible/roles/common/templates/limits.conf.j2 ===

* soft nofile 65536
* hard nofile 65536
* soft nproc 65536
* hard nproc 65536
root soft nofile 65536
root hard nofile 65536
root soft nproc 65536
root hard nproc 65536

=== ./ansible/roles/common/templates/sysctl.conf.j2 ===

# Kubernetes networking requirements
net.ipv4.ip_forward = 1
net.ipv6.conf.all.forwarding = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1

=== ./ansible/roles/common/meta/main.yml ===

dependencies: []


=== ./ansible/roles/common/handlers/main.yml ===

- name: Reload sysctl
  ansible.builtin.command:
    cmd: sysctl --system
  changed_when: true

- name: Restart chrony
  ansible.builtin.systemd:
    name: chrony
    state: restarted


=== ./ansible/roles/common/tasks/configure_firewall.yml ===

- name: Install ufw
  ansible.builtin.apt:
    name: ufw
    state: present

- name: Allow necessary ports
  ansible.builtin.ufw:
    rule: allow
    port: "{{ item.split('/')[0] }}"
    proto: "{{ item.split('/')[1] if '/' in item else 'tcp' }}"
  loop: "{{ common_firewall_ports + (cni_extra_ports[cni_plugin] if cni_plugin in cni_extra_ports else []) }}"

- name: Enable ufw
  ansible.builtin.ufw:
    state: enabled
    policy: deny


=== ./ansible/roles/common/tasks/setup_prerequisites.yml ===


- name: Install common packages
  ansible.builtin.apt:
    name: "{{ common_packages }}"
    state: present
    update_cache: yes

- name: Ensure sysctl.conf template is applied
  ansible.builtin.template:
    src: sysctl.conf.j2
    dest: /etc/sysctl.conf
    owner: root
    group: root
    mode: '0644'
  notify: Reload sysctl

- name: Ensure limits.conf template is applied
  ansible.builtin.template:
    src: limits.conf.j2
    dest: /etc/security/limits.conf
    owner: root
    group: root
    mode: '0644'

- name: Ensure swap is disabled
  ansible.builtin.command:
    cmd: swapoff -a
  changed_when: false

- name: Comment out swap entries in fstab
  ansible.builtin.replace:
    path: /etc/fstab
    regexp: '^([^#].*?\sswap\s+.*)$'
    replace: '# \1'

- name: Ensure chrony is installed
  ansible.builtin.apt:
    name: "{{ ntp_package }}"
    state: present

- name: Ensure chrony is enabled and started
  ansible.builtin.systemd:
    name: chrony
    enabled: yes
    state: started

=== ./ansible/roles/common/tasks/main.yml ===

- name: Setup system prerequisites
  include_tasks: setup_prerequisites.yml

- name: Configure firewall
  include_tasks: configure_firewall.yml


=== ./ansible/roles/container-runtime/defaults/main.yml ===

# Container runtime paths
containerd_binary_dest: /usr/local/bin/containerd
runc_binary_dest: /usr/local/bin/runc
containerd_shim_binary_dest: /usr/local/bin/containerd-shim-runc-v2
ctr_binary_dest: /usr/local/bin/ctr
containerd_config: /etc/containerd/config.toml
containerd_service: /etc/systemd/system/containerd.service
containerd_package: containerd.io


=== ./ansible/roles/container-runtime/templates/containerd-config.toml.j2 ===

version = 2
[plugins]
  [plugins."io.containerd.grpc.v1.cri"]
    sandbox_image = "registry.k8s.io/pause:3.9"
    [plugins."io.containerd.grpc.v1.cri".containerd]
      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]
        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          runtime_type = "io.containerd.runc.v2"
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            SystemdCgroup = true

=== ./ansible/roles/container-runtime/meta/main.yml ===

dependencies: []


=== ./ansible/roles/container-runtime/handlers/main.yml ===

- name: Restart containerd
  ansible.builtin.systemd:
    name: containerd
    state: restarted


=== ./ansible/roles/container-runtime/tasks/configure_containerd.yml ===

- name: Copy containerd binary
  ansible.builtin.copy:
    src: "{{ containerd_binary }}"
    dest: "{{ containerd_binary_dest }}"
    mode: "0755"

- name: Copy containerd-shim-runc-v2 binary
  ansible.builtin.copy:
    src: "{{ containerd_binary | dirname }}/containerd-shim-runc-v2"
    dest: "{{ containerd_shim_binary_dest }}"
    mode: "0755"

- name: Copy runc binary
  ansible.builtin.copy:
    src: "{{ runc_binary }}"
    dest: "{{ runc_binary_dest }}"
    mode: "0755"

- name: Copy ctr binary
  ansible.builtin.copy:
    src: "{{ containerd_binary | dirname }}/ctr"
    dest: "{{ ctr_binary_dest }}"
    mode: "0755"

- name: Ensure containerd configuration directory exists
  ansible.builtin.file:
    path: /etc/containerd
    state: directory
    owner: root
    group: root
    mode: "0755"

- name: Apply containerd configuration
  ansible.builtin.template:
    src: containerd-config.toml.j2
    dest: "{{ containerd_config }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart containerd

- name: Ensure containerd service file exists
  ansible.builtin.copy:
    content: |
      [Unit]
      Description=containerd container runtime
      Documentation=https://containerd.io
      After=network.target

      [Service]
      ExecStart={{ containerd_binary_dest }} --config {{ containerd_config }}
      Restart=on-failure
      LimitNOFILE=65536

      [Install]
      WantedBy=multi-user.target
    dest: "{{ containerd_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart containerd

- name: Ensure containerd service is enabled and started
  ansible.builtin.systemd:
    name: containerd
    enabled: yes
    state: started


=== ./ansible/roles/container-runtime/tasks/main.yml ===

- name: Configure containerd
  include_tasks: configure_containerd.yml


=== ./ansible/roles/etcd/defaults/main.yml ===

# etcd paths
etcd_binary_dest: /usr/local/bin/etcd
etcdctl_binary_dest: /usr/local/bin/etcdctl
etcd_config: /etc/etcd/etcd.conf
etcd_service: /etc/systemd/system/etcd.service


=== ./ansible/roles/etcd/templates/etcd.conf.j2 ===

ETCD_ARGS="{% for arg in etcd_args %} {{ arg }}{% endfor %}"

=== ./ansible/roles/etcd/templates/etcd.service.j2 ===

[Unit]
Description=etcd
Documentation=https://etcd.io/docs/
After=network.target

[Service]
EnvironmentFile={{ etcd_config }}
ExecStart={{ etcd_binary_dest }} $ETCD_ARGS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

=== ./ansible/roles/etcd/meta/main.yml ===

dependencies: []

=== ./ansible/roles/etcd/handlers/main.yml ===

- name: Restart etcd
  ansible.builtin.systemd:
    name: etcd
    state: restarted


=== ./ansible/roles/etcd/tasks/service.yml ===

- name: Apply etcd service template
  ansible.builtin.template:
    src: etcd.service.j2
    dest: "{{ etcd_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart etcd

- name: Ensure etcd service is enabled and started
  ansible.builtin.systemd:
    name: etcd
    enabled: yes
    state: started


=== ./ansible/roles/etcd/tasks/main.yml ===

- name: Configure etcd
  include_tasks: configure.yml

- name: Set up etcd service
  include_tasks: service.yml


=== ./ansible/roles/etcd/tasks/configure.yml ===

- name: Copy etcd binary
  ansible.builtin.copy:
    src: "{{ etcd_binary }}"
    dest: "{{ etcd_binary_dest }}"
    mode: "0755"

- name: Copy etcdctl binary
  ansible.builtin.copy:
    src: "{{ etcdctl_binary }}"
    dest: "{{ etcdctl_binary_dest }}"
    mode: "0755"

- name: Ensure etcd data directory exists
  ansible.builtin.file:
    path: "{{ etcd_data_dir }}"
    state: directory
    owner: root
    group: root
    mode: "0700"

- name: Apply etcd configuration
  ansible.builtin.template:
    src: etcd.conf.j2
    dest: "{{ etcd_config }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart etcd


=== ./ansible/roles/control-plane/defaults/main.yml ===

# Control plane paths
apiserver_binary_dest: /usr/local/bin/kube-apiserver
controller_manager_binary_dest: /usr/local/bin/kube-controller-manager
scheduler_binary_dest: /usr/local/bin/kube-scheduler
apiserver_service: /etc/systemd/system/kube-apiserver.service
controller_manager_service: /etc/systemd/system/kube-controller-manager.service
scheduler_service: /etc/systemd/system/kube-scheduler.service
encryption_config: "{{ config_dir }}/encryption-config.yaml"
kubernetes_cert_file: apiserver.pem
kubernetes_key_file: apiserver-key.pem


=== ./ansible/roles/control-plane/templates/kube-apiserver.service.j2 ===

[Unit]
Description=Kubernetes API Server
Documentation=https://kubernetes.io/docs/
After=network.target

[Service]
ExecStart={{ apiserver_binary_dest }} \
{% for arg in kube_apiserver_args %}
    {{ arg }} \
{% endfor %}
    --bind-address={{ kube_apiserver_bind_address }} \
    --secure-port={{ kube_apiserver_port }}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target


=== ./ansible/roles/control-plane/templates/kube-apiserver.yaml.j2 ===

apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - name: kube-apiserver
    image: k8s.gcr.io/kube-apiserver:v{{ kubernetes_version }}
    command:
    - kube-apiserver
    - --advertise-address={{ kubernetes_public_address }}
    - --bind-address={{ kube_apiserver_bind_address }}
    - --secure-port={{ kube_apiserver_port }}
    {% for arg in kube_apiserver_args %}
    - {{ arg }}
    {% endfor %}
    volumeMounts:
    - name: pki
      mountPath: {{ pki_dir }}
      readOnly: true
    - name: config
      mountPath: {{ config_dir }}
      readOnly: true
  volumes:
  - name: pki
    hostPath:
      path: {{ pki_dir }}
  - name: config
    hostPath:
      path: {{ config_dir }}

=== ./ansible/roles/control-plane/templates/kube-controller-manager.service.j2 ===

[Unit]
Description=Kubernetes Controller Manager
Documentation=https://kubernetes.io/docs/
After=network.target

[Service]
ExecStart={{ controller_manager_binary_dest }} \
    {% for arg in kube_controller_manager_args %} {{ arg }} {% endfor %}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

=== ./ansible/roles/control-plane/templates/encryption-config.yaml.j2 ===


apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: "{{ lookup('password', '/dev/shm/encryption_key chars=ascii_letters,digits length=32') | b64encode }}"
      - identity: {}


=== ./ansible/roles/control-plane/templates/kube-scheduler.service.j2 ===

[Unit]
Description=Kubernetes Scheduler
Documentation=https://kubernetes.io/docs/
After=network.target

[Service]
ExecStart={{ scheduler_binary_dest }} \
    {% for arg in kube_scheduler_args %} {{ arg }} {% endfor %}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

=== ./ansible/roles/control-plane/meta/main.yml ===

dependencies: []


=== ./ansible/roles/control-plane/handlers/main.yml ===

# ansible/roles/control-plane/handlers/main.yml
- name: Restart kube-apiserver
  ansible.builtin.systemd:
    name: kube-apiserver
    state: restarted

- name: Restart kube-controller-manager
  ansible.builtin.systemd:
    name: kube-controller-manager
    state: restarted

- name: Restart kube-scheduler
  ansible.builtin.systemd:
    name: kube-scheduler
    state: restarted




=== ./ansible/roles/control-plane/tasks/install_scheduler.yml ===

- name: Copy kube-scheduler binary
  ansible.builtin.copy:
    src: "{{ kube_scheduler_binary }}"
    dest: "{{ scheduler_binary_dest }}"
    mode: "0755"

- name: Apply kube-scheduler service template
  ansible.builtin.template:
    src: kube-scheduler.service.j2
    dest: "{{ scheduler_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kube-scheduler

- name: Ensure kube-scheduler service is enabled and started
  ansible.builtin.systemd:
    name: kube-scheduler
    enabled: yes
    state: started


=== ./ansible/roles/control-plane/tasks/install_apiserver.yml ===

- name: Copy kube-apiserver binary
  ansible.builtin.copy:
    src: "{{ kube_apiserver_binary }}"
    dest: "{{ apiserver_binary_dest }}"
    mode: "0755"

- name: Apply kube-apiserver service template
  ansible.builtin.template:
    src: kube-apiserver.service.j2
    dest: "{{ apiserver_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kube-apiserver

- name: Apply encryption configuration
  ansible.builtin.template:
    src: encryption-config.yaml.j2
    dest: "{{ encryption_config }}"
    owner: root
    group: root
    mode: "0600"

- name: Ensure kube-apiserver service is enabled and started
  ansible.builtin.systemd:
    name: kube-apiserver
    enabled: yes
    state: started


=== ./ansible/roles/control-plane/tasks/main.yml ===

- name: Install kube-apiserver
  include_tasks: install_apiserver.yml

- name: Install kube-controller-manager
  include_tasks: install_controller_manager.yml

- name: Install kube-scheduler
  include_tasks: install_scheduler.yml


=== ./ansible/roles/control-plane/tasks/install_controller_manager.yml ===

- name: Copy kube-controller-manager binary
  ansible.builtin.copy:
    src: "{{ kube_controller_manager_binary }}"
    dest: "{{ controller_manager_binary_dest }}"
    mode: "0755"

- name: Apply kube-controller-manager service template
  ansible.builtin.template:
    src: kube-controller-manager.service.j2
    dest: "{{ controller_manager_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kube-controller-manager

- name: Ensure kube-controller-manager service is enabled and started
  ansible.builtin.systemd:
    name: kube-controller-manager
    enabled: yes
    state: started


=== ./ansible/roles/kube-proxy/defaults/main.yml ===

# Kube-proxy paths
kube_proxy_binary_dest: /usr/local/bin/kube-proxy
kube_proxy_config: "{{ config_dir }}/kube-proxy-config.yaml"
kube_proxy_service: /etc/systemd/system/kube-proxy.service


=== ./ansible/roles/kube-proxy/templates/kube-proxy.service.j2 ===

[Unit]
Description=Kubernetes Kube-Proxy
Documentation=https://kubernetes.io/docs/
After=network.target

[Service]
ExecStart={{ kube_proxy_binary_dest }} \
    {% for arg in kube_proxy_args %} {{ arg }} {% endfor %}
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

=== ./ansible/roles/kube-proxy/templates/kube-proxy-config.yaml.j2 ===

apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: iptables
clusterCIDR: "{{ pod_network_cidr }}"

=== ./ansible/roles/kube-proxy/meta/main.yml ===

dependencies: []


=== ./ansible/roles/kube-proxy/handlers/main.yml ===

- name: Restart kube-proxy
  ansible.builtin.systemd:
    name: kube-proxy
    state: restarted


=== ./ansible/roles/kube-proxy/tasks/main.yml ===

- name: Configure kube-proxy
  include_tasks: configure_kube_proxy.yml


=== ./ansible/roles/kube-proxy/tasks/configure_kube_proxy.yml ===

- name: Copy kube-proxy binary
  ansible.builtin.copy:
    src: "{{ kube_proxy_binary }}"
    dest: "{{ kube_proxy_binary_dest }}"
    mode: "0755"

- name: Apply kube-proxy configuration
  ansible.builtin.template:
    src: kube-proxy-config.yaml.j2
    dest: "{{ kube_proxy_config }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kube-proxy

- name: Apply kube-proxy service template
  ansible.builtin.template:
    src: kube-proxy.service.j2
    dest: "{{ kube_proxy_service }}"
    owner: root
    group: root
    mode: "0644"
  notify: Restart kube-proxy

- name: Ensure kube-proxy service is enabled and started
  ansible.builtin.systemd:
    name: kube-proxy
    enabled: yes
    state: started


=== ./ansible/roles/cni/defaults/main.yml ===

# Flannel paths
cni_manifest: "{{ paths.artifacts.manifests }}/kube-flannel.yml"


=== ./ansible/roles/cni/meta/main.yml ===

dependencies: []


=== ./ansible/roles/cni/handlers/main.yml ===

- name: Apply CNI
  ansible.builtin.command:
    cmd: "kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig apply -f /etc/kubernetes/{{ cni_plugin }}.yml"
  changed_when: true


=== ./ansible/roles/cni/tasks/main.yml ===

- name: Copy CNI manifest
  ansible.builtin.copy:
    src: "{{ cni_manifest_paths[cni_plugin] }}"
    dest: "/etc/kubernetes/{{ cni_plugin }}.yml"
    owner: root
    group: root
    mode: "0644"
  when: inventory_hostname == 'master1'

- name: Apply CNI manifest
  ansible.builtin.command:
    cmd: "kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig apply -f /etc/kubernetes/{{ cni_plugin }}.yml"
    creates: /var/run/cni-applied
  changed_when: true
  when: inventory_hostname == 'master1'
  notify: Apply CNI


=== ./ansible/roles/bootstrap/defaults/main.yml ===

# Bootstrap settings
bootstrap_token: "{{ lookup('password', '/dev/shm/bootstrap_token chars=ascii_lowercase,digits length=16') }}"
bootstrap_token_id: "{{ bootstrap_token[:6] }}"
bootstrap_token_secret: "{{ bootstrap_token[6:] }}"
bootstrap_kubeconfig: "{{ config_dir }}/bootstrap.kubeconfig"


=== ./ansible/roles/bootstrap/templates/bootstrap.kubeconfig.j2 ===

apiVersion: v1
kind: Config
clusters:
- cluster:
    certificate-authority: {{ pki_dir }}/ca.pem
    server: https://192.168.56.11:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: bootstrap
  name: bootstrap@kubernetes
current-context: bootstrap@kubernetes
users:
- name: bootstrap
  user:
    token: "{{ bootstrap_token_id }}.{{ bootstrap_token_secret }}"

=== ./ansible/roles/bootstrap/meta/main.yml ===

dependencies: []


=== ./ansible/roles/bootstrap/handlers/main.yml ===

# - name: Approve CSRs
#   ansible.builtin.command:
#     cmd: "kubectl certificate approve {{ item }}"
#   loop: "{{ q('kubernetes.core.k8s', api_version='certificates.k8s.io/v1', kind='CertificateSigningRequest') | json_query('[].metadata.name') }}"
#   changed_when: true
#   when: inventory_hostname == 'master1'

- name: Approve CSRs
  ansible.builtin.shell: |
    kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig get csr -o name | xargs -r kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig certificate approve
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/roles/bootstrap/tasks/main.yml ===

- name: Configure bootstrap
  include_tasks: configure_bootstrap.yml


=== ./ansible/roles/bootstrap/tasks/configure_bootstrap.yml ===

- name: Check if bootstrap token secret exists
  ansible.builtin.command:
    cmd: kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig -n kube-system get secret bootstrap-token-{{ bootstrap_token_id }}
  register: secret_check
  failed_when: false
  changed_when: false
  when: inventory_hostname == 'master1' and not ansible_check_mode

- name: Create bootstrap token
  ansible.builtin.command:
    cmd: "kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig -n kube-system create secret generic bootstrap-token-{{ bootstrap_token_id }} --type=bootstrap.kubernetes.io/token --from-literal=token-id={{ bootstrap_token_id }} --from-literal=token-secret={{ bootstrap_token_secret }} --from-literal=usage-bootstrap-authentication=true --from-literal=usage-bootstrap-signing=true"
  changed_when: true
  when: inventory_hostname == 'master1' and not ansible_check_mode and secret_check.rc != 0

- name: Create bootstrap kubeconfig
  ansible.builtin.template:
    src: bootstrap.kubeconfig.j2
    dest: "{{ bootstrap_kubeconfig }}"
    owner: root
    group: root
    mode: "0600"
  when: inventory_hostname in groups['workers']

- name: Wait for and approve all pending CSRs
  ansible.builtin.shell: |
    # Wait up to 60 seconds for CSRs to appear
    end_time=$(( $(date +%s) + 60 ))
    while [ $(date +%s) -lt $end_time ]; do
      if kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig get csr -o name 2>/dev/null | grep -q .; then
        break
      fi
      sleep 2
    done

    # Approve all pending CSRs
    kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig get csr -o name | xargs -r kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig certificate approve
  args:
    executable: /bin/bash
  changed_when: true
  when: inventory_hostname == 'master1'
  register: csr_result
  failed_when:
    - csr_result.rc != 0
    - "'No resources found' not in csr_result.stderr"


=== ./ansible/roles/hardening/defaults/main.yml ===

audit_policy: "{{ config_dir }}/audit-policy.yaml"
network_policy: "{{ config_dir }}/default-network-policy.yaml"
rbac_config: "{{ config_dir }}/rbac-config.yaml"
etcd_backup_dir: /var/backups/etcd
etcd_backup_script: /usr/local/bin/etcd-backup.sh


=== ./ansible/roles/hardening/templates/default-network-policy.yaml.j2 ===

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  egress:
  - ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 53
    to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns

=== ./ansible/roles/hardening/templates/rbac-config.yaml.j2 ===

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: restricted-cluster-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: User
  name: admin
  apiGroup: rbac.authorization.k8s.io

=== ./ansible/roles/hardening/meta/main.yml ===

dependencies: []


=== ./ansible/roles/hardening/handlers/main.yml ===

- name: Apply RBAC
  ansible.builtin.command:
    cmd: "kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig apply -f {{ rbac_config }}"
  changed_when: true

- name: Apply network policy
  ansible.builtin.command:
    cmd: "kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig apply -f {{ network_policy }}"
  changed_when: true


=== ./ansible/roles/hardening/tasks/remove_bootstrap.yml ===

- name: Remove bootstrap token secrets
  ansible.builtin.shell: |
    # Get all bootstrap token secrets
    secrets=$(kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig -n kube-system get secret -o name | grep bootstrap-token || true)

    # Delete each secret individually
    if [ -n "$secrets" ]; then
      for secret in $secrets; do
        # Extract just the secret name (remove 'secret/' prefix)
        secret_name=$(echo "$secret" | sed 's|^secret/||')
        kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig -n kube-system delete secret "$secret_name" --ignore-not-found
      done
    fi
  args:
    executable: /bin/bash
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/roles/hardening/tasks/main.yml ===

- name: Remove bootstrap tokens
  include_tasks: remove_bootstrap.yml

- name: Configure audit policy
  include_tasks: configure_audit.yml

- name: Configure network policies
  include_tasks: network_policies.yml

- name: Configure RBAC
  include_tasks: configure_rbac.yml

- name: Configure etcd backup
  include_tasks: etcd_backup.yml


=== ./ansible/roles/hardening/tasks/configure_audit.yml ===

- name: Apply audit policy
  ansible.builtin.copy:
    content: |
      apiVersion: audit.k8s.io/v1
      kind: Policy
      rules:
      - level: Metadata
    dest: "{{ audit_policy }}"
    owner: root
    group: root
    mode: "0644"
  when: inventory_hostname in groups['masters']


=== ./ansible/roles/hardening/tasks/configure_rbac.yml ===

- name: Apply RBAC configuration
  ansible.builtin.template:
    src: rbac-config.yaml.j2
    dest: "{{ rbac_config }}"
    owner: root
    group: root
    mode: "0644"
  notify: Apply RBAC
  when: inventory_hostname == 'master1'

- name: Apply RBAC policy
  ansible.builtin.command:
    cmd: "kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig apply -f {{ rbac_config }}"
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/roles/hardening/tasks/network_policies.yml ===

- name: Apply default network policy
  ansible.builtin.template:
    src: default-network-policy.yaml.j2
    dest: "{{ network_policy }}"
    owner: root
    group: root
    mode: "0644"
  notify: Apply network policy
  when: inventory_hostname == 'master1'

- name: Apply network policy
  ansible.builtin.command:
    cmd: "kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig apply -f {{ network_policy }}"
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/roles/hardening/tasks/etcd_backup.yml ===

- name: Ensure etcd backup directory exists
  ansible.builtin.file:
    path: "{{ etcd_backup_dir }}"
    state: directory
    owner: root
    group: root
    mode: "0700"
  when: inventory_hostname in groups['etcd']

- name: Create etcd backup script
  ansible.builtin.copy:
    content: |
      #!/bin/bash
      ETCDCTL_API=3 {{ etcdctl_binary }} snapshot save {{ etcd_backup_dir }}/etcd-snapshot-$(date +%Y%m%d%H%M%S).db \
        --cacert={{ pki_dir }}/ca.pem \
        --cert={{ pki_dir }}/etcd-server.pem \
        --key={{ pki_dir }}/etcd-server-key.pem \
        --endpoints=https://{{ kubernetes_public_address }}:2379
    dest: "{{ etcd_backup_script }}"
    owner: root
    group: root
    mode: "0755"
  when: inventory_hostname in groups['etcd']

- name: Schedule etcd backup cron job
  ansible.builtin.cron:
    name: etcd-backup
    minute: "0"
    hour: "2"
    job: "{{ etcd_backup_script }}"
  when: inventory_hostname in groups['etcd']


=== ./ansible/roles/jumpbox_setup/meta/main.yml ===

dependencies: []


=== ./ansible/roles/jumpbox_setup/tasks/main.yml ===



=== ./ansible/roles/jumpbox_setup/tasks/configure_kubectl.yml ===

- name: Ensure config directory exists on jumpbox
  ansible.builtin.file:
    path: "{{ config_dir }}"
    state: directory
    owner: root
    group: root
    mode: "0755"
  when: inventory_hostname == 'jumpbox' and not ansible_check_mode

- name: Copy admin kubeconfig from master1 to jumpbox
  ansible.builtin.copy:
    src: "{{ config_dir }}/admin.kubeconfig"
    dest: "{{ config_dir }}/admin.kubeconfig"
    owner: root
    group: root
    mode: "0600"
    remote_src: true
  when: inventory_hostname == 'jumpbox' and not ansible_check_mode
  delegate_to: master1

- name: Verify admin kubeconfig exists on jumpbox
  ansible.builtin.stat:
    path: "{{ config_dir }}/admin.kubeconfig"
  register: kubeconfig_stat
  when: inventory_hostname == 'jumpbox' and not ansible_check_mode

- name: Fail if admin kubeconfig is missing
  ansible.builtin.fail:
    msg: "admin.kubeconfig not found at {{ config_dir }}/admin.kubeconfig on jumpbox"
  when: inventory_hostname == 'jumpbox' and not ansible_check_mode and not kubeconfig_stat.stat.exists

- name: Verify kubectl can connect to cluster
  ansible.builtin.command:
    cmd: kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig get nodes
  register: kubectl_check
  failed_when: false
  changed_when: false
  when: inventory_hostname == 'jumpbox' and not ansible_check_mode

- name: Display cluster connection status
  ansible.builtin.debug:
    msg: |
      kubectl connection status: {% if kubectl_check is defined and kubectl_check.rc is defined %}
        {% if kubectl_check.rc == 0 %}
          Connected successfully. Output: {{ kubectl_check.stdout }}
        {% else %}
          Failed to connect. Error: {{ kubectl_check.stderr }}
        {% endif %}
      {% else %}
        Check command did not run or failed to register.
      {% endif %}
  when: inventory_hostname == 'jumpbox' and not ansible_check_mode


=== ./ansible/roles/networking/defaults/main.yml ===

# Networking paths
coredns_manifest: "{{ config_dir }}/coredns-deployment.yaml"
kubectl_binary_dest: /usr/local/bin/kubectl


=== ./ansible/roles/networking/templates/coredns-deployment.yaml.j2 ===

apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
spec:
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  selector:
    k8s-app: kube-dns
  clusterIP: "{{ service_cidr | ipaddr('10') | ipaddr('address') }}"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
spec:
  replicas: 2
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      containers:
      - name: coredns
        image: coredns/coredns:1.11.3
        args:
        - -conf
        - /etc/coredns/Corefile
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
      volumes:
      - name: config-volume
        configMap:
          name: coredns
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local {{ pod_network_cidr | ipaddr('net') }}
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
    }

=== ./ansible/roles/networking/meta/main.yml ===

dependencies: []


=== ./ansible/roles/networking/handlers/main.yml ===

- name: apply_coredns
  ansible.builtin.command:
    cmd: "kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig apply -f {{ coredns_manifest }}"
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/roles/networking/tasks/main.yml ===

- name: Deploy CoreDNS
  include_tasks: deploy_coredns.yml


=== ./ansible/roles/networking/tasks/deploy_coredns.yml ===

- name: Apply CoreDNS manifest
  ansible.builtin.template:
    src: coredns-deployment.yaml.j2
    dest: "{{ coredns_manifest }}"
    owner: root
    group: root
    mode: "0644"
  notify: apply_coredns
  when: inventory_hostname == 'master1'

- name: Apply CoreDNS configuration
  ansible.builtin.command:
    cmd: "kubectl --kubeconfig={{ config_dir }}/admin.kubeconfig apply -f {{ coredns_manifest }}"
  changed_when: true
  when: inventory_hostname == 'master1'


=== ./ansible/roles/distribute-artifacts/defaults/main.yml ===

# Role-specific defaults for artifact distribution
distribute_timeout: 300
distribute_retries: 3


=== ./ansible/roles/distribute-artifacts/meta/main.yml ===

dependencies:
  - generate-artifacts # Requires certs and kubeconfigs
galaxy_info:
  role_name: distribute-artifacts
  author: yourname
  description: Distribute binaries, certificates, and kubeconfigs to Kubernetes nodes
  license: MIT


=== ./ansible/roles/distribute-artifacts/tasks/distribute-certs.yml ===

- name: Ensure PKI directory exists on nodes
  file:
    path: "{{ paths.kubernetes.pki_dir }}"
    state: directory
    owner: root
    group: root
    mode: "0700"

- name: Distribute CA certificate to all nodes
  copy:
    src: "{{ paths.artifacts.certs }}/{{ item }}"
    dest: "{{ paths.kubernetes.pki_dir }}/{{ item }}"
    mode: "0644"
    owner: root
    group: root
  loop:
    - ca.pem
    - ca-key.pem
  when: inventory_hostname in groups['masters'] or inventory_hostname in groups['workers']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute API server certificates to masters
  copy:
    src: "{{ paths.artifacts.certs }}/{{ item }}"
    dest: "{{ paths.kubernetes.pki_dir }}/{{ item }}"
    mode: "0600"
    owner: root
    group: root
  loop:
    - apiserver.pem
    - apiserver-key.pem
  when: inventory_hostname in groups['masters']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute controller manager certificates to masters
  copy:
    src: "{{ paths.artifacts.certs }}/{{ item }}"
    dest: "{{ paths.kubernetes.pki_dir }}/{{ item }}"
    mode: "0600"
    owner: root
    group: root
  loop:
    - kube-controller-manager.pem
    - kube-controller-manager-key.pem
  when: inventory_hostname in groups['masters']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute scheduler certificates to masters
  copy:
    src: "{{ paths.artifacts.certs }}/{{ item }}"
    dest: "{{ paths.kubernetes.pki_dir }}/{{ item }}"
    mode: "0600"
    owner: root
    group: root
  loop:
    - kube-scheduler.pem
    - kube-scheduler-key.pem
  when: inventory_hostname in groups['masters']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute service account keys to masters
  copy:
    src: "{{ paths.artifacts.certs }}/{{ item }}"
    dest: "{{ paths.kubernetes.pki_dir }}/{{ item }}"
    mode: "0600"
    owner: root
    group: root
  loop:
    - sa.pub
    - sa-key.pem
  when: inventory_hostname in groups['masters']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute etcd certificates to masters
  copy:
    src: "{{ paths.artifacts.certs }}/{{ item }}"
    dest: "{{ paths.etcd.cert_dir }}/{{ item }}"
    mode: "0600"
    owner: root
    group: root
  loop:
    - etcd-server.pem
    - etcd-server-key.pem
    - apiserver-etcd-client.pem
    - apiserver-etcd-client-key.pem
  when: inventory_hostname in groups['masters']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute kubelet certificates to nodes
  copy:
    src: "{{ paths.artifacts.certs }}/{{ inventory_hostname }}.pem"
    dest: "{{ paths.kubernetes.pki_dir }}/kubelet.pem"
    mode: "0600"
    owner: root
    group: root
  when: inventory_hostname in groups['masters'] or inventory_hostname in groups['workers']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute kubelet private key to nodes
  copy:
    src: "{{ paths.artifacts.certs }}/kubelet-key.pem"
    dest: "{{ paths.kubernetes.pki_dir }}/kubelet-key.pem"
    mode: "0600"
    owner: root
    group: root
  when: inventory_hostname in groups['masters'] or inventory_hostname in groups['workers']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute kube-proxy certificates to workers
  copy:
    src: "{{ paths.artifacts.certs }}/{{ item }}"
    dest: "{{ paths.kubernetes.pki_dir }}/{{ item }}"
    mode: "0600"
    owner: root
    group: root
  loop:
    - kube-proxy.pem
    - kube-proxy-key.pem
  when: inventory_hostname in groups['workers']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost


=== ./ansible/roles/distribute-artifacts/tasks/main.yml ===

- name: Distribute binaries
  include_tasks: distribute-binaries.yml

- name: Distribute certificates
  include_tasks: distribute-certs.yml

- name: Distribute kubeconfigs
  include_tasks: distribute-kubeconfigs.yml

- name: Distribute manifests
  include_tasks: distribute-manifests.yml


=== ./ansible/roles/distribute-artifacts/tasks/distribute-kubeconfigs.yml ===

- name: Ensure config directory exists on nodes
  file:
    path: "{{ paths.kubernetes.config_dir }}"
    state: directory
    owner: root
    group: root
    mode: "0700"

- name: Distribute admin kubeconfig to masters
  copy:
    src: "{{ paths.artifacts.kubeconfigs }}/admin.kubeconfig"
    dest: "{{ paths.kubernetes.config_dir }}/admin.kubeconfig"
    mode: "0600"
    owner: root
    group: root
  when: inventory_hostname in groups['masters']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute admin kubeconfig to jumpbox
  copy:
    src: "{{ paths.artifacts.kubeconfigs }}/admin.kubeconfig"
    dest: "{{ paths.kubernetes.config_dir }}/admin.kubeconfig"
    mode: "0600"
    owner: root
    group: root
  when: inventory_hostname == 'jumpbox'
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute kubelet kubeconfig to nodes
  copy:
    src: "{{ paths.artifacts.kubeconfigs }}/{{ inventory_hostname }}.kubeconfig"
    dest: "{{ paths.kubernetes.config_dir }}/kubelet.kubeconfig"
    mode: "0600"
    owner: root
    group: root
  when: inventory_hostname in groups['masters'] or inventory_hostname in groups['workers']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute kube-proxy kubeconfig to workers
  copy:
    src: "{{ paths.artifacts.kubeconfigs }}/kube-proxy.kubeconfig"
    dest: "{{ paths.kubernetes.config_dir }}/kube-proxy.kubeconfig"
    mode: "0600"
    owner: root
    group: root
  when: inventory_hostname in groups['workers']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost


=== ./ansible/roles/distribute-artifacts/tasks/distribute-binaries.yml ===

- name: Ensure binary destination directory exists on nodes
  file:
    path: "/usr/local/bin"
    state: directory
    owner: root
    group: root
    mode: "0755"

- name: Distribute Kubernetes binaries to masters
  copy:
    src: "{{ paths.artifacts.binaries }}/{{ item }}"
    dest: "/usr/local/bin/{{ item }}"
    mode: "0755"
    owner: root
    group: root
  loop:
    - kube-apiserver
    - kube-controller-manager
    - kube-scheduler
    - kubectl
  when: inventory_hostname in groups['masters']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute kubectl to jumpbox
  copy:
    src: "{{ paths.artifacts.binaries }}/kubectl"
    dest: "/usr/local/bin/kubectl"
    mode: "0755"
    owner: root
    group: root
  when: inventory_hostname == 'jumpbox'
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute Kubernetes binaries to workers
  copy:
    src: "{{ paths.artifacts.binaries }}/{{ item }}"
    dest: "/usr/local/bin/{{ item }}"
    mode: "0755"
    owner: root
    group: root
  loop:
    - kubelet
    - kube-proxy
    - kubectl
  when: inventory_hostname in groups['workers']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute etcd binaries to masters
  copy:
    src: "{{ paths.artifacts.binaries }}/{{ item }}"
    dest: "/usr/local/bin/{{ item }}"
    mode: "0755"
    owner: root
    group: root
  loop:
    - etcd
    - etcdctl
  when: inventory_hostname in groups['masters']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute containerd binaries to all nodes
  copy:
    src: "{{ paths.artifacts.binaries }}/{{ item }}"
    dest: "/usr/local/bin/{{ item }}"
    mode: "0755"
    owner: root
    group: root
  loop:
    - containerd
    - containerd-shim-runc-v2
    - ctr
  when: inventory_hostname in groups['masters'] or inventory_hostname in groups['workers']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute runc binary to all nodes
  copy:
    src: "{{ paths.artifacts.binaries }}/runc"
    dest: "/usr/local/bin/runc"
    mode: "0755"
    owner: root
    group: root
  when: inventory_hostname in groups['masters'] or inventory_hostname in groups['workers']
  retries: "{{ distribute_retries }}"
  delegate_to: localhost


=== ./ansible/roles/distribute-artifacts/tasks/distribute-manifests.yml ===

- name: Ensure manifests directory exists on masters
  file:
    path: "{{ paths.kubernetes.manifests_dir }}"
    state: directory
    owner: root
    group: root
    mode: "0755"
  when: inventory_hostname in groups['masters']

- name: Distribute Flannel manifest to masters
  copy:
    src: "{{ paths.artifacts.manifests }}/kube-flannel.yml"
    dest: "{{ paths.kubernetes.manifests_dir }}/kube-flannel.yml"
    mode: "0644"
    owner: root
    group: root
  when: inventory_hostname in groups['masters'] and cni_plugin == 'flannel'
  retries: "{{ distribute_retries }}"
  delegate_to: localhost

- name: Distribute Calico manifest to masters
  copy:
    src: "{{ paths.artifacts.manifests }}/calico.yaml"
    dest: "{{ paths.kubernetes.manifests_dir }}/calico.yaml"
    mode: "0644"
    owner: root
    group: root
  when: inventory_hostname in groups['masters'] and cni_plugin == 'calico'
  retries: "{{ distribute_retries }}"
  delegate_to: localhost


=== ./ansible/roles/download-artifacts/defaults/main.yml ===

download_timeout: 300
download_retries: 3


=== ./ansible/roles/download-artifacts/meta/main.yml ===

dependencies: []


=== ./ansible/roles/download-artifacts/tasks/main.yml ===

- name: Setup artifact directories
  include: directory_setup.yml

- name: Download kubernetes binaries
  include: kubernetes_binaries.yml

- name: Download containerd binaries
  include: containerd_binaries.yml

- name: Download etcd binaries
  include: etcd_binaries.yml

- name: Download runc binaries
  include: runc_binaries.yml

- name: Download flannel and calico yaml files
  include: flannel_and_calico.yml

- name: Cleanup unncessary files
  include: cleanup.yml


=== ./ansible/roles/download-artifacts/tasks/cleanup.yml ===

- name: Clean up containerd unnecessary directories
  ansible.builtin.file:
    path: "{{ paths.artifacts.binaries }}/bin"
    state: absent
  when: inventory_hostname == 'jumpbox'
  delegate_to: localhost

- name: Clean up containerd unnecessary files
  ansible.builtin.file:
    path: "{{ paths.artifacts.binaries }}/containerd-stress"
    state: absent
  when: inventory_hostname == 'jumpbox'
  delegate_to: localhost

- name: Clean up etcd unnecessary directories
  ansible.builtin.file:
    path: "{{ paths.artifacts.binaries }}/etcd-v{{ versions.etcd }}-linux-amd64"
    state: absent
  when: inventory_hostname == 'jumpbox'
  delegate_to: localhost

- name: Clean up etcd unnecessary files
  ansible.builtin.file:
    path: "{{ paths.artifacts.binaries }}/{{ item }}"
    state: absent
  loop:
    - README-etcdctl.md
    - README-etcdutl.md
    - README.md
    - READMEv2-etcdctl.md
    - etcdutl
  when: inventory_hostname == 'jumpbox'
  delegate_to: localhost


=== ./ansible/roles/download-artifacts/tasks/flannel_and_calico.yml ===

- name: Download Flannel manifest
  get_url:
    url: "{{ download_urls.manifests.flannel }}"
    dest: "{{ paths.artifacts.manifests }}/kube-flannel.yml"
    mode: "0644"
    owner: vagrant
    group: vagrant
    timeout: "{{ download_timeout }}"
  retries: "{{ download_retries }}"
  delegate_to: localhost

- name: Download Calico manifest
  get_url:
    url: "{{ download_urls.manifests.calico }}"
    dest: "{{ paths.artifacts.manifests }}/calico.yaml"
    mode: "0644"
    owner: vagrant
    group: vagrant
    timeout: "{{ download_timeout }}"
  retries: "{{ download_retries }}"
  delegate_to: localhost


=== ./ansible/roles/download-artifacts/tasks/directory_setup.yml ===

- name: Ensure artifacts base directory exists
  file:
    path: "{{ paths.artifacts.base }}"
    state: directory
    owner: vagrant
    group: vagrant
    mode: "0755"
  delegate_to: localhost

- name: Create artifacts subdirectories
  file:
    path: "{{ item }}"
    state: directory
    owner: vagrant
    group: vagrant
    mode: "0755"
  loop:
    - "{{ paths.artifacts.binaries }}"
    - "{{ paths.artifacts.certs }}"
    - "{{ paths.artifacts.kubeconfigs }}"
    - "{{ paths.artifacts.manifests }}"
  delegate_to: localhost


=== ./ansible/roles/download-artifacts/tasks/kubernetes_binaries.yml ===

- name: Download Kubernetes binaries with checksums
  get_url:
    url: "{{ item.url }}"
    dest: "{{ paths.artifacts.binaries }}/{{ item.dest }}"
    mode: "0755"
    owner: vagrant
    group: vagrant
    checksum: "{{ item.checksum }}"
    timeout: "{{ download_timeout }}"
  loop:
    - {
        url: "{{ download_urls.kubernetes.apiserver }}",
        dest: "kube-apiserver",
        checksum: "{{ checksums.kubernetes.apiserver }}",
      }
    - {
        url: "{{ download_urls.kubernetes.controller_manager }}",
        dest: "kube-controller-manager",
        checksum: "{{ checksums.kubernetes.controller_manager }}",
      }
    - {
        url: "{{ download_urls.kubernetes.scheduler }}",
        dest: "kube-scheduler",
        checksum: "{{ checksums.kubernetes.scheduler }}",
      }
    - {
        url: "{{ download_urls.kubernetes.kubelet }}",
        dest: "kubelet",
        checksum: "{{ checksums.kubernetes.kubelet }}",
      }
    - {
        url: "{{ download_urls.kubernetes.kube_proxy }}",
        dest: "kube-proxy",
        checksum: "{{ checksums.kubernetes.kube_proxy }}",
      }
    - {
        url: "{{ download_urls.kubernetes.kubectl }}",
        dest: "kubectl",
        checksum: "{{ checksums.kubernetes.kubectl }}",
      }
  retries: "{{ download_retries }}"
  delegate_to: localhost


=== ./ansible/roles/download-artifacts/tasks/etcd_binaries.yml ===

- name: Download etcd tarball with checksum
  get_url:
    url: "{{ download_urls.etcd.tarball }}"
    dest: "{{ paths.artifacts.binaries }}/etcd-v{{ versions.etcd }}-linux-amd64.tar.gz"
    mode: "0644"
    checksum: "{{ checksums.etcd.tarball }}"
    timeout: "{{ download_timeout }}"
  retries: "{{ download_retries }}"
  delegate_to: localhost

- name: Extract etcd tarball
  unarchive:
    src: "{{ paths.artifacts.binaries }}/etcd-v{{ versions.etcd }}-linux-amd64.tar.gz"
    dest: "{{ paths.artifacts.binaries }}"
    remote_src: yes
    creates: "{{ paths.artifacts.binaries }}/etcd-v{{ versions.etcd }}-linux-amd64/etcd"
  delegate_to: localhost

- name: Move etcd binaries and set permissions
  copy:
    src: "{{ paths.artifacts.binaries }}/etcd-v{{ versions.etcd }}-linux-amd64/{{ item }}"
    dest: "{{ paths.artifacts.binaries }}/{{ item }}"
    remote_src: yes
    mode: "0755"
    owner: vagrant
    group: vagrant
  loop:
    - etcd
    - etcdctl
  delegate_to: localhost


=== ./ansible/roles/download-artifacts/tasks/containerd_binaries.yml ===

- name: Download containerd tarball with checksum
  get_url:
    url: "{{ download_urls.containerd.tarball }}"
    dest: "{{ paths.artifacts.binaries }}/containerd-{{ versions.containerd }}-linux-amd64.tar.gz"
    mode: "0644"
    checksum: "{{ checksums.containerd.tarball }}"
    timeout: "{{ download_timeout }}"
  retries: "{{ download_retries }}"
  delegate_to: localhost

- name: Extract containerd tarball
  unarchive:
    src: "{{ paths.artifacts.binaries }}/containerd-{{ versions.containerd }}-linux-amd64.tar.gz"
    dest: "{{ paths.artifacts.binaries }}"
    remote_src: yes
    creates: "{{ paths.artifacts.binaries }}/bin/containerd"
  delegate_to: localhost

- name: Move containerd binaries and set permissions
  copy:
    src: "{{ paths.artifacts.binaries }}/bin/{{ item }}"
    dest: "{{ paths.artifacts.binaries }}/{{ item }}"
    remote_src: yes
    mode: "0755"
    owner: vagrant
    group: vagrant
  loop:
    - containerd
    - containerd-shim
    - containerd-shim-runc-v1
    - containerd-shim-runc-v2
    - ctr
  delegate_to: localhost


=== ./ansible/roles/download-artifacts/tasks/runc_binaries.yml ===

- name: Download runc with checksum
  get_url:
    url: "{{ download_urls.runc.binary }}"
    dest: "{{ paths.artifacts.binaries }}/runc"
    mode: "0755"
    owner: vagrant
    group: vagrant
    checksum: "{{ checksums.runc.binary }}"
    timeout: "{{ download_timeout }}"
  retries: "{{ download_retries }}"
  delegate_to: localhost


=== ./ansible/ansible.cfg ===

[defaults]
inventory = inventory/hosts.ini
remote_user = vagrant
roles_path = roles
private_key_file = /home/vagrant/.ssh/id_rsa
host_key_checking = False
retry_files_enabled = False
stdout_callback = yaml

[privilege_escalation]
become = True
become_method = sudo
become_user = root
become_ask_pass = False

=== ./provision_jumpbox.sh ===

#!/bin/bash

set -e  # Exit on error

# Function for formatted task output
task_echo() {
    echo "===> $1"
}

task_echo "[Task 1] - Install required packages"
{
    apt-get update
    apt-get install -y ansible
}

task_echo "[Task 2] - Verify directory structure"
{
    echo "Current directory: $(pwd)"
    echo "Listing /home/vagrant/k8s-project:"
    ls -la /home/vagrant/k8s-project/
    echo "Checking ansible directory:"
    ls -la /home/vagrant/k8s-project/ansible/ || echo "Ansible directory not found!"
}

task_echo "[Task 3] - Run Ansible playbook for jumpbox setup"
{
    # Check if ansible directory exists
    if [ -d "/home/vagrant/k8s-project/ansible" ]; then
        cd /home/vagrant/k8s-project/ansible
        ansible-playbook -i inventory/hosts.ini playbooks/jumpbox_setup.yml --connection=local -l jumpbox
    else
        echo "ERROR: Ansible directory not found at /home/vagrant/k8s-project/ansible/"
        echo "Available content in /home/vagrant/k8s-project/:"
        ls -la /home/vagrant/k8s-project/
        exit 1
    fi
}

echo "Jumpbox provisioning complete."

=== ./scripts/check_requirements.sh ===

#!/bin/bash

# Define cluster resource requirements for 1 master, 2 workers, 1 jumpbox
REQ_CPU_CORES=6           # 1 master (2 CPUs) + 2 workers (1 CPU each) + 1 jumpbox (1 CPU) + 1 buffer
REQ_RAM_MB=7168           # 1 master (3072MB) + 2 workers (2048MB each) + 1 jumpbox (512MB) + 10% buffer
REQ_DISK_GB=60            # 1 master (20GB) + 2 workers (20GB each) + 1 jumpbox (10GB) + 10% buffer

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
NC='\033[0m' # No Color

echo "Checking system requirements for 1-master, 2-worker, 1-jumpbox Kubernetes cluster..."

# Initialize variables for status tracking
CPU_STATUS="PASS"
RAM_STATUS="PASS"
DISK_STATUS="PASS"

# 1. CPU Check
# Install sysstat if not present (for mpstat)
if ! command -v mpstat &> /dev/null; then
    echo "Installing sysstat for mpstat..."
    sudo apt-get update && sudo apt-get install -y sysstat
fi

TOTAL_CPU_CORES=$(nproc)
# Get idle percentage from mpstat (single snapshot)
IDLE_PERCENT=$(mpstat 1 1 | grep -A 1 "all" | tail -1 | awk '{print $NF}')
USED_PERCENT=$(echo "100 - $IDLE_PERCENT" | bc -l)
USED_CPU_CORES=$(echo "$USED_PERCENT * $TOTAL_CPU_CORES / 100" | bc -l | awk '{printf "%.1f", $0}')
AVAILABLE_CPU_CORES=$(echo "$IDLE_PERCENT * $TOTAL_CPU_CORES / 100" | bc -l | awk '{printf "%.1f", $0}')
echo "CPU Check:"
echo "  Total Capacity: ${TOTAL_CPU_CORES} cores"
echo "  Used: ${USED_CPU_CORES} cores (${USED_PERCENT}%)"
echo "  Available: ${AVAILABLE_CPU_CORES} cores (${IDLE_PERCENT}%)"
echo "  Required: ${REQ_CPU_CORES} cores"
if [ "$(echo "$AVAILABLE_CPU_CORES >= $REQ_CPU_CORES" | bc -l)" -eq 1 ]; then
    echo -e "  Status: ${GREEN}PASS${NC}"
else
    echo -e "  Status: ${RED}FAIL - Need at least ${REQ_CPU_CORES} cores available${NC}"
    CPU_STATUS="FAIL"
fi
echo ""

# 2. RAM Check
TOTAL_RAM_MB=$(free -m | awk '/Mem:/ {print $2}')
USED_RAM_MB=$(free -m | awk '/Mem:/ {print $3}')
AVAILABLE_RAM_MB=$(free -m | awk '/Mem:/ {print $4}')
echo "RAM Check:"
echo "  Total Capacity: ${TOTAL_RAM_MB}MB"
echo "  Used: ${USED_RAM_MB}MB"
echo "  Available: ${AVAILABLE_RAM_MB}MB"
echo "  Required: ${REQ_RAM_MB}MB"
if [ "$AVAILABLE_RAM_MB" -ge "$REQ_RAM_MB" ]; then
    echo -e "  Status: ${GREEN}PASS${NC}"
else
    echo -e "  Status: ${RED}FAIL - Need at least ${REQ_RAM_MB}MB free RAM${NC}"
    RAM_STATUS="FAIL"
fi
echo ""

# 3. Disk Check
DISK_PATH="$HOME/VirtualBox VMs"
if [ ! -d "$DISK_PATH" ]; then
    DISK_PATH="$HOME"
fi
TOTAL_DISK_GB=$(df -h "$DISK_PATH" | tail -1 | awk '{print int($2)}')
USED_DISK_GB=$(df -h "$DISK_PATH" | tail -1 | awk '{print int($3)}')
AVAILABLE_DISK_GB=$(df -h "$DISK_PATH" | tail -1 | awk '{print int($4)}')
echo "Disk Check (at $DISK_PATH):"
echo "  Total Capacity: ${TOTAL_DISK_GB}GB"
echo "  Used: ${USED_DISK_GB}GB"
echo "  Available: ${AVAILABLE_DISK_GB}GB"
echo "  Required: ${REQ_DISK_GB}GB"
if [ "$AVAILABLE_DISK_GB" -ge "$REQ_DISK_GB" ]; then
    echo -e "  Status: ${GREEN}PASS${NC}"
else
    echo -e "  Status: ${RED}FAIL - Need at least ${REQ_DISK_GB}GB free disk space${NC}"
    DISK_STATUS="FAIL"
fi
echo ""

# Summary
echo "Summary of Requirements Check:"
echo -e "  CPU: ${CPU_STATUS}"
echo -e "  RAM: ${RAM_STATUS}"
echo -e "  Disk: ${DISK_STATUS}"
echo ""

if [ "$CPU_STATUS" = "PASS" ] && [ "$RAM_STATUS" = "PASS" ] && [ "$DISK_STATUS" = "PASS" ]; then
    echo -e "${GREEN}All checks passed! Your system meets the requirements for the Kubernetes cluster setup.${NC}"
else
    echo -e "${RED}One or more checks failed. Please ensure your system meets the minimum requirements before proceeding.${NC}"
    exit 1
fi


=== ./scripts/distribute-ssh-keys.sh ===

#!/bin/bash

set -e

echo "=== Setting up SSH keys for cluster access ==="

# Use the mounted Vagrant insecure keys
VAGRANT_KEY="/home/vagrant/vagrant-keys/vagrant.key.rsa"

if [ ! -f "$VAGRANT_KEY" ]; then
    echo "ERROR: Vagrant RSA key not found at $VAGRANT_KEY"
    exit 1
fi

echo "Using Vagrant key: $VAGRANT_KEY"

# Generate new SSH key pair on jumpbox
echo "Generating new SSH key pair..."
sudo -u vagrant mkdir -p /home/vagrant/.ssh
sudo -u vagrant ssh-keygen -t rsa -b 4096 -f /home/vagrant/.ssh/id_rsa -N "" -q
sudo -u vagrant chmod 600 /home/vagrant/.ssh/id_rsa

# Read the public key
PUBLIC_KEY=$(sudo -u vagrant cat /home/vagrant/.ssh/id_rsa.pub)

# Distribute the public key to all nodes using Vagrant's insecure key
for IP in 192.168.56.11 192.168.56.21 192.168.56.22; do
    echo "Distributing key to $IP..."
    
    # Use Vagrant's key to SSH into each node and add our public key
    ssh -i "$VAGRANT_KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null vagrant@$IP "
        mkdir -p /home/vagrant/.ssh
        echo '$PUBLIC_KEY' >> /home/vagrant/.ssh/authorized_keys
        chmod 700 /home/vagrant/.ssh
        chmod 600 /home/vagrant/.ssh/authorized_keys
        echo 'Key deployed to $IP'
    "
done

# Also add the key to jumpbox itself
echo "$PUBLIC_KEY" >> /home/vagrant/.ssh/authorized_keys
chmod 600 /home/vagrant/.ssh/authorized_keys

echo "=== SSH setup complete ==="
echo "New SSH key generated and distributed to all nodes"

